#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨é¢„æµ‹åˆ†ææ¨¡å—
åŸºäºKronosæ¨¡å‹è¿›è¡Œè‚¡ç¥¨é¢„æµ‹å¹¶ä¿å­˜ç»“æœ
"""

import pandas as pd
import numpy as np
import os
import sys
import json
import argparse
from datetime import datetime, timedelta
import torch
import warnings
import logging
from typing import Optional, Dict, List, Tuple, Any
warnings.filterwarnings('ignore')

# å»¶è¿Ÿå¯¼å…¥matplotlibï¼Œé¿å…åˆå§‹åŒ–å†²çª
# import matplotlib.pyplot as plt  # å°†åœ¨éœ€è¦æ—¶å¯¼å…¥

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

try:
    from model import Kronos, KronosTokenizer, KronosPredictor
except ImportError:
    print("é”™è¯¯: æ— æ³•å¯¼å…¥Kronosæ¨¡å‹ï¼Œè¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨")
    sys.exit(1)

class StockPredictor:
    """è‚¡ç¥¨é¢„æµ‹å™¨"""
    
    def __init__(self, device="auto", max_context=512, results_dir="prediction_results", enable_adaptive_tuning=True):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨

        Args:
            device (str): è®¡ç®—è®¾å¤‡ï¼Œå¯ä¸º 'cpu'ã€'cuda:0' æˆ– 'auto'
            max_context (int): æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            results_dir (str): ç»“æœä¿å­˜ç›®å½•
            enable_adaptive_tuning (bool): æ˜¯å¦å¯ç”¨è‡ªé€‚åº”å‚æ•°è°ƒä¼˜
        """
        # åˆå§‹åŒ–æ—¥å¿—
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        self.requested_device = device
        self.device = self._resolve_device(device)
        self.max_context = max_context
        self.results_dir = results_dir
        self.enable_adaptive_tuning = enable_adaptive_tuning

        # æ¨¡å‹ç›¸å…³
        self.model = None
        self.tokenizer = None
        self.predictor = None

        # æ€§èƒ½ç›‘æ§
        self.performance_stats = {
            'predictions_count': 0,
            'total_inference_time': 0,
            'memory_peak': 0,
            'errors_count': 0
        }
        
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        self.ensure_results_dir()
        
        # åˆå§‹åŒ–æ¨¡å‹
        try:
            self.load_model()
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–Kronosæ¨¡å‹: {str(e)}")

        # æ•°æ®é¢„å¤„ç†é…ç½®
        self.data_config = {
            'required_columns': ['open', 'high', 'low', 'close', 'volume', 'amount'],
            'timestamp_column': 'timestamps',
            'max_nan_ratio': 0.05,  # æœ€å¤§å…è®¸çš„NaNæ¯”ä¾‹
            'min_data_points': 100,  # æœ€å°‘æ•°æ®ç‚¹æ•°
            'outlier_threshold': 3.0  # å¼‚å¸¸å€¼æ£€æµ‹é˜ˆå€¼(æ ‡å‡†å·®å€æ•°)
        }

        self.logger.info("StockPredictoråˆå§‹åŒ–å®Œæˆ")

    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return self.performance_stats.copy()

    def reset_performance_stats(self):
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats = {
            'predictions_count': 0,
            'total_inference_time': 0,
            'memory_peak': 0,
            'errors_count': 0
        }

    def optimize_memory_usage(self):
        """å†…å­˜ä¼˜åŒ–"""
        import gc

        try:
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()

            # å¦‚æœä½¿ç”¨GPUï¼Œæ¸…ç©ºç¼“å­˜
            if self.device.startswith('cuda'):
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        self.logger.info("å·²æ¸…ç©ºGPUç¼“å­˜")
                except ImportError:
                    pass
            elif self.device == 'mps':
                try:
                    import torch
                    if torch.backends.mps.is_available():
                        torch.mps.empty_cache()
                        self.logger.info("å·²æ¸…ç©ºMPSç¼“å­˜")
                except ImportError:
                    pass

        except Exception as e:
            self.logger.warning(f"å†…å­˜ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºç°è­¦å‘Š: {str(e)}")
    
    def ensure_results_dir(self):
        """ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.results_dir):
            os.makedirs(self.results_dir)
            print(f"åˆ›å»ºç»“æœç›®å½•: {self.results_dir}")
    
    def _resolve_device(self, device):
        """æ ¹æ®å½“å‰ç¯å¢ƒè§£æå®é™…ä½¿ç”¨çš„è®¾å¤‡"""
        normalized = (device or "auto").lower()

        if normalized == "auto":
            if torch.backends.mps.is_available() and torch.backends.mps.is_built():
                print("âœ… æ£€æµ‹åˆ° Apple Silicon MPSï¼Œè‡ªåŠ¨ä½¿ç”¨ 'mps' è®¾å¤‡ã€‚")
                return "mps"
            elif torch.cuda.is_available():
                print("âœ… æ£€æµ‹åˆ°å¯ç”¨çš„ CUDAï¼Œè‡ªåŠ¨ä½¿ç”¨ 'cuda:0' è®¾å¤‡ã€‚")
                return "cuda:0"
            else:
                print("â„¹ï¸ æœªæ£€æµ‹åˆ° GPU åŠ é€Ÿï¼Œå°†ä½¿ç”¨ CPUã€‚")
                return "cpu"

        if normalized.startswith("cuda"):
            if torch.cuda.is_available():
                return device
            print("âš ï¸ è¯·æ±‚ä½¿ç”¨ CUDAï¼Œä½†å½“å‰ç¯å¢ƒä¸æ”¯æŒï¼Œå·²è‡ªåŠ¨å›é€€åˆ° CPUã€‚")
            return "cpu"

        if normalized == "mps":
            if torch.backends.mps.is_available() and torch.backends.mps.is_built():
                return "mps"
            print("âš ï¸ è¯·æ±‚ä½¿ç”¨ MPS (Apple Silicon)ï¼Œä½†å½“å‰ç¯å¢ƒä¸æ”¯æŒï¼Œå·²è‡ªåŠ¨å›é€€åˆ° CPUã€‚")
            return "cpu"

        return device
    
    def validate_data(self, df: pd.DataFrame, context: str = "general") -> Tuple[bool, str]:
        """
        éªŒè¯æ•°æ®è´¨é‡å’Œå®Œæ•´æ€§

        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            context: éªŒè¯ä¸Šä¸‹æ–‡æè¿°

        Returns:
            (is_valid, error_message)
        """
        try:
            # 1. æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
            missing_cols = set(self.data_config['required_columns']) - set(df.columns)
            if missing_cols:
                return False, f"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}"

            # 2. æ£€æŸ¥æ—¶é—´æˆ³åˆ—
            if self.data_config['timestamp_column'] not in df.columns:
                return False, f"ç¼ºå°‘æ—¶é—´æˆ³åˆ—: {self.data_config['timestamp_column']}"

            # 3. æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
            if len(df) < self.data_config['min_data_points']:
                return False, f"æ•°æ®ç‚¹è¿‡å°‘: {len(df)} < {self.data_config['min_data_points']}"

            # 4. æ£€æŸ¥NaNå€¼æ¯”ä¾‹
            nan_ratio = df[self.data_config['required_columns']].isnull().sum().sum() / (len(df) * len(self.data_config['required_columns']))
            if nan_ratio > self.data_config['max_nan_ratio']:
                return False, f"NaNå€¼æ¯”ä¾‹è¿‡é«˜: {nan_ratio:.2%} > {self.data_config['max_nan_ratio']:.2%}"

            # 5. æ£€æŸ¥ä»·æ ¼æ•°æ®åˆç†æ€§
            price_cols = ['open', 'high', 'low', 'close']
            for col in price_cols:
                if (df[col] <= 0).any():
                    return False, f"å‘ç°éæ­£ä»·æ ¼å€¼åœ¨åˆ— {col}"

                # æ£€æŸ¥high >= max(open, close), low <= min(open, close)
                if col == 'high':
                    invalid_high = df['high'] < df[['open', 'close']].max(axis=1)
                    if invalid_high.any():
                        return False, f"å‘ç°highä»·æ ¼ä½äºopenæˆ–close"
                elif col == 'low':
                    invalid_low = df['low'] > df[['open', 'close']].min(axis=1)
                    if invalid_low.any():
                        return False, f"å‘ç°lowä»·æ ¼é«˜äºopenæˆ–close"

            # 6. æ£€æŸ¥æ—¶é—´æˆ³æ’åº
            if not df[self.data_config['timestamp_column']].is_monotonic_increasing:
                return False, "æ—¶é—´æˆ³ä¸æ˜¯å•è°ƒé€’å¢çš„"

            return True, "æ•°æ®éªŒè¯é€šè¿‡"

        except Exception as e:
            return False, f"æ•°æ®éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"

    def preprocess_data(self, df: pd.DataFrame, detect_outliers: bool = True) -> pd.DataFrame:
        """
        æ•°æ®é¢„å¤„ç†å’Œæ¸…ç†

        Args:
            df: åŸå§‹æ•°æ®æ¡†
            detect_outliers: æ˜¯å¦æ£€æµ‹å’Œå¤„ç†å¼‚å¸¸å€¼

        Returns:
            å¤„ç†åçš„æ•°æ®æ¡†
        """
        try:
            logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")

            # é¦–å…ˆéªŒè¯è¾“å…¥æ•°æ®
            is_valid, error_msg = self.validate_data(df, "preprocessing_input")
            if not is_valid:
                raise ValueError(f"è¾“å…¥æ•°æ®éªŒè¯å¤±è´¥: {error_msg}")

            processed_df = df.copy()

            # 1. å¤„ç†ç¼ºå¤±å€¼
            numeric_cols = self.data_config['required_columns']
            for col in numeric_cols:
                if processed_df[col].isnull().any():
                    # ä½¿ç”¨å‰å‘å¡«å……ï¼Œç„¶ååå‘å¡«å……
                    processed_df[col] = processed_df[col].ffill().bfill()
                    # å¦‚æœä»æœ‰NaNï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……
                    if processed_df[col].isnull().any():
                        median_val = processed_df[col].median()
                        processed_df[col] = processed_df[col].fillna(median_val)
                        logger.warning(f"åˆ— {col} ä½¿ç”¨ä¸­ä½æ•° {median_val:.2f} å¡«å……å‰©ä½™NaNå€¼")

            # 2. å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†
            if detect_outliers:
                processed_df = self._handle_outliers(processed_df)

            # 3. æ·»åŠ æ•°æ®å¹³æ»‘å¤„ç†ï¼ˆä¼˜åŒ–æªæ–½ï¼‰
            processed_df = self._smooth_price_data(processed_df)

            # 4. ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            processed_df[self.data_config['timestamp_column']] = pd.to_datetime(processed_df[self.data_config['timestamp_column']])
            for col in numeric_cols:
                processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')

            # 5. æ’åºæ•°æ®
            processed_df = processed_df.sort_values(self.data_config['timestamp_column']).reset_index(drop=True)

            # 6. æœ€ç»ˆéªŒè¯å¤„ç†åçš„æ•°æ®
            is_valid, error_msg = self.validate_data(processed_df, "preprocessing_output")
            if not is_valid:
                logger.warning(f"é¢„å¤„ç†åæ•°æ®å­˜åœ¨é—®é¢˜: {error_msg}ï¼Œä½†ç»§ç»­æ‰§è¡Œ")

            logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œå¤„ç†åæ•°æ®é‡: {len(processed_df)}")
            return processed_df

        except Exception as e:
            logger.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}")
            raise

    def _smooth_price_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å¯¹ä»·æ ¼æ•°æ®è¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œå‡å°‘å™ªå£°ï¼Œæé«˜é¢„æµ‹ç¨³å®šæ€§
        """
        smoothed_df = df.copy()
        price_cols = ['open', 'high', 'low', 'close']

        for col in price_cols:
            if col in smoothed_df.columns:
                # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡è¿›è¡Œè½»å¾®å¹³æ»‘ï¼ˆalpha=0.1ï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰
                smoothed_df[col] = smoothed_df[col].ewm(alpha=0.1, adjust=True).mean()

        logger.info("å·²åº”ç”¨æ•°æ®å¹³æ»‘å¤„ç†ä»¥æé«˜é¢„æµ‹ç¨³å®šæ€§")
        return smoothed_df

    def _handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å¤„ç†å¼‚å¸¸å€¼ - ä½¿ç”¨IQRæ–¹æ³•è¿›è¡Œæ›´robustçš„æ£€æµ‹
        """
        processed_df = df.copy()
        price_cols = ['open', 'high', 'low', 'close']

        for col in price_cols:
            # ä½¿ç”¨IQR (å››åˆ†ä½è·) æ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼ï¼Œæ›´é€‚åˆè‚¡ç¥¨æ•°æ®
            Q1 = processed_df[col].quantile(0.25)
            Q3 = processed_df[col].quantile(0.75)
            IQR = Q3 - Q1

            # IQRå¼‚å¸¸å€¼æ£€æµ‹ï¼šä½äºQ1-1.5*IQRæˆ–é«˜äºQ3+1.5*IQRçš„å€¼
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outlier_mask = (processed_df[col] < lower_bound) | (processed_df[col] > upper_bound)

            if outlier_mask.any():
                outlier_count = outlier_mask.sum()
                logger.warning(f"åˆ— {col} ä½¿ç”¨IQRæ–¹æ³•å‘ç° {outlier_count} ä¸ªå¼‚å¸¸å€¼")

                # å¯¹äºä»·æ ¼æ•°æ®ï¼Œä½¿ç”¨å±€éƒ¨å›å½’æˆ–æ’å€¼è€Œä¸æ˜¯ç®€å•çš„ç§»åŠ¨å¹³å‡
                if outlier_count / len(processed_df) < 0.05:  # å¼‚å¸¸å€¼æ¯”ä¾‹å°äº5%
                    # ä½¿ç”¨çº¿æ€§æ’å€¼æ›¿æ¢å¼‚å¸¸å€¼
                    processed_df[col] = processed_df[col].where(~outlier_mask, np.nan)
                    processed_df[col] = processed_df[col].interpolate(method='linear', limit_direction='both')
                else:
                    # å¦‚æœå¼‚å¸¸å€¼å¤ªå¤šï¼Œä½¿ç”¨ç§»åŠ¨ä¸­ä½æ•°ï¼ˆæ›´robustï¼‰
                    processed_df[col] = processed_df[col].where(~outlier_mask, processed_df[col].rolling(window=7, center=True, min_periods=3).median())

                # ç¡®ä¿æ’å€¼åæ²¡æœ‰NaNå€¼
                if processed_df[col].isnull().any():
                    processed_df[col] = processed_df[col].ffill().bfill()

        return processed_df
    
    def load_model(self):
        """åŠ è½½Kronosæ¨¡å‹"""
        try:
            print(f"æ­£åœ¨åŠ è½½Kronosæ¨¡å‹... (device: {self.device})")

            # è®¾ç½®ç¯å¢ƒå˜é‡è§£å†³SSLé—®é¢˜
            import os
            os.environ['HF_HUB_DISABLE_SSL_VERIFICATION'] = '1'
            os.environ['REQUESTS_CA_BUNDLE'] = ''
            os.environ['SSL_CERT_FILE'] = ''

            # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
            self.tokenizer = KronosTokenizer.from_pretrained("NeoQuasar/Kronos-Tokenizer-base")
            self.model = Kronos.from_pretrained("NeoQuasar/Kronos-base")

            # åˆ›å»ºé¢„æµ‹å™¨
            self.predictor = KronosPredictor(
                self.model,
                self.tokenizer,
                device=self.device,
                max_context=self.max_context
            )

            print("âœ… Kronoså¤§æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            print(f"   ğŸ“Š æ¨¡å‹ä¿¡æ¯: {self.model.__class__.__name__}")
            print(f"   ğŸ–¥ï¸  è¿è¡Œè®¾å¤‡: {self.device}")
            print(f"   ğŸ§  æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦: {self.max_context}")

        except ImportError as e:
            print(f"âŒ æ¨¡å‹å¯¼å…¥å¤±è´¥: {e}")
            print("\nğŸ”§ è§£å†³æ–¹æ¡ˆ:")
            print("  1. ç¡®ä¿å·²å®‰è£…Kronosç›¸å…³ä¾èµ–: pip install -r requirements.txt")
            print("  2. æ£€æŸ¥modelç›®å½•æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«å¿…è¦çš„æ–‡ä»¶")
            raise RuntimeError(f"Kronosæ¨¡å‹å¯¼å…¥å¤±è´¥: {e}")

        except ConnectionError as e:
            print(f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥: {e}")
            print("\nğŸ”§ è§£å†³æ–¹æ¡ˆ:")
            print("  1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            print("  2. è®¾ç½®ä»£ç†: export HTTPS_PROXY=http://your-proxy:port")
            print("  3. æˆ–ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°åä½¿ç”¨ç¦»çº¿æ¨¡å¼")
            raise RuntimeError(f"æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥: {e}")

        except Exception as e:
            error_type = type(e).__name__
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ ({error_type}): {e}")
            print("\nğŸ”§ è§£å†³æ–¹æ¡ˆ:")
            if "SSL" in str(e).upper():
                print("  1. SSLè¯ä¹¦é—®é¢˜ï¼Œå°è¯•: pip install --upgrade requests urllib3")
                print("  2. æˆ–è®¾ç½®ç¯å¢ƒå˜é‡è·³è¿‡SSLéªŒè¯: export HF_HUB_DISABLE_SSL_VERIFICATION=1")
            elif "timeout" in str(e).lower():
                print("  1. ç½‘ç»œè¶…æ—¶ï¼Œæ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä½¿ç”¨ä»£ç†")
                print("  2. æˆ–ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°åä½¿ç”¨ç¦»çº¿æ¨¡å¼")
            elif "disk" in str(e).lower():
                print("  1. ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œæ¸…ç†ç£ç›˜ç©ºé—´")
                print("  2. æˆ–è®¾ç½®HF_HOMEåˆ°å…¶ä»–ç›®å½•")
            else:
                print("  1. æ£€æŸ¥HuggingFace tokenæ˜¯å¦æ­£ç¡®è®¾ç½®")
                print("  2. å°è¯•é‡æ–°å®‰è£…transformersåº“")
                print("  3. æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†…å­˜å’Œç£ç›˜ç©ºé—´")
            raise RuntimeError(f"Kronosæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def load_data(self, filepath):
        """
        åŠ è½½è‚¡ç¥¨æ•°æ®
        
        Args:
            filepath (str): æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            pd.DataFrame: è‚¡ç¥¨æ•°æ®
        """
        try:
            print(f"æ­£åœ¨åŠ è½½æ•°æ®: {filepath}")
            
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(filepath)
            
            # ç¡®ä¿æ—¶é—´æˆ³åˆ—å­˜åœ¨
            if 'timestamps' not in df.columns:
                print("é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸­ç¼ºå°‘timestampsåˆ—")
                return None
            
            # è½¬æ¢æ—¶é—´æˆ³æ ¼å¼
            df['timestamps'] = pd.to_datetime(df['timestamps'])
            
            # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„åˆ—éƒ½å­˜åœ¨
            required_columns = ['timestamps', 'open', 'high', 'low', 'close', 'volume', 'amount']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                print(f"è­¦å‘Š: ç¼ºå°‘åˆ—: {missing_columns}")
                # å°è¯•ç”¨å…¶ä»–åˆ—ååŒ¹é…
                column_mapping = {
                    'Open': 'open',
                    'High': 'high',
                    'Low': 'low',
                    'Close': 'close',
                    'Volume': 'volume',
                    'Amount': 'amount'
                }
                
                for old_col, new_col in column_mapping.items():
                    if old_col in df.columns and new_col not in df.columns:
                        df[new_col] = df[old_col]
            
            # æ•°æ®ç±»å‹è½¬æ¢
            numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'amount']
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # åˆ é™¤æ— æ•ˆæ•°æ®
            df = df.dropna()
            
            # æŒ‰æ—¶é—´æ’åº
            df = df.sort_values('timestamps').reset_index(drop=True)
            
            print(f"æˆåŠŸåŠ è½½ {len(df)} æ¡æ•°æ®")
            print(f"æ—¶é—´èŒƒå›´: {df['timestamps'].min()} åˆ° {df['timestamps'].max()}")
            
            return df
            
        except Exception as e:
            print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    def prepare_prediction_data(self, df, lookback=1500, pred_len=96):
        """
        å‡†å¤‡é¢„æµ‹æ•°æ®
        
        Args:
            df (pd.DataFrame): è‚¡ç¥¨æ•°æ®
            lookback (int): å›çœ‹çª—å£å¤§å°
            pred_len (int): é¢„æµ‹é•¿åº¦
            
        Returns:
            tuple: (è¾“å…¥æ•°æ®, è¾“å…¥æ—¶é—´æˆ³, è¾“å‡ºæ—¶é—´æˆ³)
        """
        if len(df) < lookback + pred_len:
            print(f"è­¦å‘Š: æ•°æ®é•¿åº¦({len(df)})å°äºlookback({lookback}) + pred_len({pred_len})")
            # è°ƒæ•´å‚æ•°
            lookback = min(lookback, len(df) // 2)
            pred_len = min(pred_len, len(df) - lookback)
            print(f"è°ƒæ•´å‚æ•°: lookback={lookback}, pred_len={pred_len}")
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        x_df = df.loc[:lookback-1, ['open', 'high', 'low', 'close', 'volume', 'amount']]
        x_timestamp = df.loc[:lookback-1, 'timestamps']
        y_timestamp = df.loc[lookback:lookback+pred_len-1, 'timestamps']
        
        return x_df, x_timestamp, y_timestamp
    
    def predict(self, x_df, x_timestamp, y_timestamp, pred_len, T=0.3, top_p=0.8, sample_count=5):
        """
        è¿›è¡Œé¢„æµ‹
        
        Args:
            x_df (pd.DataFrame): è¾“å…¥ç‰¹å¾
            x_timestamp (pd.Series): è¾“å…¥æ—¶é—´æˆ³
            y_timestamp (pd.Series): é¢„æµ‹æ—¶é—´æˆ³
            pred_len (int): é¢„æµ‹é•¿åº¦
            T (float): é‡‡æ ·æ¸©åº¦ ä¸¾ä¾‹ï¼šè‹¥é¢„æµ‹ â€œæ˜å¤©æ˜¯å¦ä¸‹é›¨â€ï¼ŒT=0.1 æ—¶ï¼Œæ¨¡å‹ä¼šåšå®šé€‰æ‹©è®­ç»ƒæ•°æ®ä¸­æœ€å¯èƒ½çš„ç»“æœï¼ˆå¦‚ â€œä¸‹é›¨â€ æ¦‚ç‡ 80% åˆ™ç›´æ¥è¾“å‡ºï¼‰ï¼›è‹¥ T è¿‡é«˜ï¼ˆå¦‚ 0.8ï¼‰ï¼Œå¯èƒ½å› éšæœºé‡‡æ ·è¾“å‡ºä½æ¦‚ç‡é€‰é¡¹ï¼ˆå¦‚ â€œæ™´å¤©â€ï¼‰ï¼Œåç¦»çœŸå®è¶‹åŠ¿ã€‚
            top_p (float): æ ¸é‡‡æ ·æ¦‚ç‡  è‹¥åœºæ™¯ä¸­ â€œçœŸç›¸â€ é«˜åº¦å”¯ä¸€ï¼ˆå¦‚é¢„æµ‹å…·ä½“æ•°å€¼ã€æ˜ç¡®åˆ†ç±»ï¼‰ï¼Œtop_p å¯é™è‡³ 0.5-0.6ï¼Œè¿›ä¸€æ­¥èšç„¦ï¼›è‹¥æ•°æ®å­˜åœ¨è½»å¾®ä¸ç¡®å®šæ€§ï¼ˆå¦‚å¤šå› ç´ å½±å“çš„é¢„æµ‹ï¼‰ï¼Œ0.7-0.8 æ›´ç¨³å¦¥ã€‚
            sample_count (int): é‡‡æ ·æ¬¡æ•°
            
        Returns:
            pd.DataFrame: é¢„æµ‹ç»“æœ
        """
        import time
        import os

        start_time = time.time()

        # å°è¯•å¯¼å…¥psutilï¼Œå¦‚æœå¤±è´¥åˆ™é™çº§åˆ°åŸºç¡€ç›‘æ§
        try:
            import psutil
            psutil_available = True
            initial_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB
        except ImportError:
            psutil_available = False
            initial_memory = 0
            self.logger.warning("psutilä¸å¯ç”¨ï¼Œæ€§èƒ½ç›‘æ§åŠŸèƒ½å°†é™çº§")

        try:
            self.logger.info("ğŸ”® å¼€å§‹æ¨¡å‹æ¨ç†...")
            self.performance_stats['predictions_count'] += 1

            # éªŒè¯è¾“å…¥å‚æ•°
            if x_df is None or x_df.empty:
                raise ValueError("è¾“å…¥æ•°æ®ä¸ºç©º")
            if len(y_timestamp) != pred_len:
                raise ValueError(f"é¢„æµ‹æ—¶é—´æˆ³é•¿åº¦({len(y_timestamp)})ä¸é¢„æµ‹é•¿åº¦({pred_len})ä¸åŒ¹é…")

            # è‡ªé€‚åº”å‚æ•°è°ƒæ•´
            T, top_p, sample_count = self._adaptive_parameter_tuning(x_df, T, top_p, sample_count, pred_len, self.enable_adaptive_tuning)

            self.logger.info(f"ä½¿ç”¨å‚æ•°: T={T:.2f}, top_p={top_p:.2f}, sample_count={sample_count}")

            # å†…å­˜ä¼˜åŒ–ï¼šåœ¨å¤§é¢„æµ‹å‰æ¸…ç†å†…å­˜
            if pred_len > 200:
                self.optimize_memory_usage()

            # æ‰§è¡Œé¢„æµ‹ - è°ƒç”¨Kronoså¤§æ¨¡å‹è¿›è¡Œæ¨ç†
            self.logger.info(f"ğŸ§  æ­£åœ¨è°ƒç”¨Kronoså¤§æ¨¡å‹è¿›è¡Œé¢„æµ‹æ¨ç† (é¢„æµ‹é•¿åº¦: {pred_len})...")
            pred_df = self.predictor.predict(
                df=x_df,
                x_timestamp=x_timestamp,
                y_timestamp=y_timestamp,
                pred_len=pred_len,
                T=T,
                top_p=top_p,
                sample_count=sample_count,
                verbose=True  # æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
            )
            self.logger.info("âœ… Kronoså¤§æ¨¡å‹æ¨ç†å®Œæˆï¼")

            if pred_df is not None:
                # åå¤„ç†é¢„æµ‹ç»“æœ
                pred_df = self._postprocess_predictions(pred_df, x_df)

                # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
                inference_time = time.time() - start_time

                if psutil_available:
                    current_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
                    memory_used = current_memory - initial_memory
                    self.performance_stats['total_inference_time'] += inference_time
                    self.performance_stats['memory_peak'] = max(self.performance_stats['memory_peak'], memory_used)
                    self.logger.info(f"âœ… é¢„æµ‹å®Œæˆå¹¶ä¼˜åŒ–ï¼ (è€—æ—¶: {inference_time:.2f}s, å†…å­˜: {memory_used:.1f}MB)")
                else:
                    self.performance_stats['total_inference_time'] += inference_time
                    self.logger.info(f"âœ… é¢„æµ‹å®Œæˆå¹¶ä¼˜åŒ–ï¼ (è€—æ—¶: {inference_time:.2f}s)")
            else:
                self.performance_stats['errors_count'] += 1
                self.logger.error("âŒ é¢„æµ‹è¿”å›None")

            return pred_df
            
        except MemoryError:
            self.logger.error("å†…å­˜ä¸è¶³ï¼Œæ— æ³•å®Œæˆé¢„æµ‹")
            self.optimize_memory_usage()
            return None
        except Exception as e:
            self.performance_stats['errors_count'] += 1
            self.logger.error(f"é¢„æµ‹å¤±è´¥: {str(e)}")
            self.logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", exc_info=True)
            return None
    
    def _adaptive_parameter_tuning(self, x_df, T, top_p, sample_count, pred_len, enable_adaptive=True):
        """
        è‡ªé€‚åº”å‚æ•°è°ƒä¼˜

        æ ¹æ®æ•°æ®ç‰¹å¾å’Œé¢„æµ‹é•¿åº¦è‡ªåŠ¨è°ƒæ•´å‚æ•°
        """
        # å¦‚æœç¦ç”¨è‡ªé€‚åº”è°ƒæ•´ï¼Œç›´æ¥è¿”å›åŸå§‹å‚æ•°
        if not enable_adaptive:
            self.logger.info("è‡ªé€‚åº”å‚æ•°è°ƒä¼˜å·²ç¦ç”¨ï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å‚æ•°")
            return T, top_p, sample_count

        # è®¡ç®—æ•°æ®çš„æ³¢åŠ¨æ€§
        close_volatility = x_df['close'].pct_change().std()

        # æ ¹æ®æ³¢åŠ¨æ€§è°ƒæ•´å‚æ•°
        if close_volatility > 0.02:  # é«˜æ³¢åŠ¨æ€§
            T = min(T * 0.8, 0.5)  # é™ä½æ¸©åº¦ï¼Œä½¿é¢„æµ‹æ›´ä¿å®ˆ
            top_p = min(top_p * 0.9, 0.6)  # é™ä½å¤šæ ·æ€§
        elif close_volatility < 0.005:  # ä½æ³¢åŠ¨æ€§
            T = max(T * 1.2, 0.3)  # å¯ä»¥é€‚å½“å¢åŠ æ¸©åº¦
            top_p = max(top_p * 1.1, 0.4)

        # æ ¹æ®é¢„æµ‹é•¿åº¦è°ƒæ•´é‡‡æ ·æ¬¡æ•°
        if pred_len > 100:  # é•¿åºåˆ—é¢„æµ‹
            sample_count = min(sample_count + 1, 5)  # å¢åŠ é‡‡æ ·æ¬¡æ•°æé«˜ç¨³å®šæ€§
        elif pred_len < 20:  # çŸ­åºåˆ—é¢„æµ‹
            sample_count = max(sample_count - 1, 1)  # å¯ä»¥å‡å°‘é‡‡æ ·æ¬¡æ•°

        # ç¡®ä¿å‚æ•°åœ¨åˆç†èŒƒå›´å†…
        T = max(0.1, min(T, 2.0))
        top_p = max(0.1, min(top_p, 0.95))
        sample_count = max(1, min(sample_count, 10))

        return T, top_p, sample_count

    def _postprocess_predictions(self, pred_df, x_df):
        """
        é¢„æµ‹ç»“æœåå¤„ç†

        ç¡®ä¿é¢„æµ‹ç»“æœçš„åˆç†æ€§å’Œè¿ç»­æ€§
        """
        processed_df = pred_df.copy()

        # 1. ç¡®ä¿ä»·æ ¼è¿ç»­æ€§ï¼ˆé˜²æ­¢è·³è·ƒï¼‰
        for col in ['open', 'high', 'low', 'close']:
            if col in processed_df.columns:
                # è®¡ç®—ç›¸é‚»é¢„æµ‹å€¼çš„å˜åŒ–ç‡
                pct_change = processed_df[col].pct_change()

                # è¯†åˆ«å¼‚å¸¸å˜åŒ–ï¼ˆè¶…è¿‡50%çš„å•æ­¥å˜åŒ–ï¼‰
                outlier_mask = pct_change.abs() > 0.5

                if outlier_mask.any():
                    logger.warning(f"æ£€æµ‹åˆ° {outlier_mask.sum()} ä¸ªå¼‚å¸¸ä»·æ ¼å˜åŒ–ï¼Œå·²è¿›è¡Œå¹³æ»‘å¤„ç†")

                    # ä½¿ç”¨ç§»åŠ¨å¹³å‡å¹³æ»‘å¼‚å¸¸å€¼
                    processed_df[col] = processed_df[col].where(
                        ~outlier_mask,
                        processed_df[col].rolling(window=3, center=True, min_periods=1).mean()
                    )

        # 2. ç¡®ä¿OHLCå…³ç³»åˆç†
        processed_df['high'] = processed_df[['open', 'close', 'high']].max(axis=1)
        processed_df['low'] = processed_df[['open', 'close', 'low']].min(axis=1)

        # 3. ç¡®ä¿æˆäº¤é‡ä¸ä¸ºè´Ÿæ•°
        if 'volume' in processed_df.columns:
            processed_df['volume'] = processed_df['volume'].clip(lower=0)
        if 'amount' in processed_df.columns:
            processed_df['amount'] = processed_df['amount'].clip(lower=0)

        return processed_df

    def _calculate_smart_xticks(self, timestamps, max_ticks=15, is_future_forecast=False):
        """
        è®¡ç®—æ™ºèƒ½æ—¶é—´è½´åˆ»åº¦ï¼Œè¶Šæ¥è¿‘å½“å‰æ—¶é—´æ˜¾ç¤ºè¶Šè¯¦ç»†

        Args:
            timestamps: æ‰€æœ‰æ—¶é—´æˆ³
            max_ticks: æœ€å¤§åˆ»åº¦æ•°é‡

        Returns:
            æ™ºèƒ½åˆ»åº¦ä½ç½®å’Œæ ‡ç­¾
        """
        if timestamps.empty:
            return [], []

        timestamps = pd.to_datetime(timestamps)
        start_time = timestamps.min()
        end_time = timestamps.max()
        total_duration = end_time - start_time

        # è®¡ç®—å…³é”®æ—¶é—´ç‚¹
        current_time = pd.Timestamp.now()
        pred_start = timestamps[timestamps >= start_time].min() if len(timestamps) > 0 else end_time

        # æ ¹æ®æ—¶é—´è·¨åº¦ç¡®å®šåˆ»åº¦å¯†åº¦
        total_hours = total_duration.total_seconds() / 3600

        if total_hours <= 24:  # 1å¤©å†…
            # æ¯å°æ—¶ä¸€ä¸ªåˆ»åº¦ï¼Œé‡ç‚¹åŒºåŸŸæ¯15åˆ†é’Ÿ
            base_freq = 'H'
            dense_freq = '15min'
        elif total_hours <= 168:  # 1å‘¨å†…
            # æ¯å¤©ä¸€ä¸ªåˆ»åº¦ï¼Œé‡ç‚¹åŒºåŸŸæ¯å°æ—¶
            base_freq = 'D'
            dense_freq = 'H'
        elif total_hours <= 720:  # 1æœˆå†…
            # æ¯å‘¨ä¸€ä¸ªåˆ»åº¦ï¼Œé‡ç‚¹åŒºåŸŸæ¯å¤©
            base_freq = 'W'
            dense_freq = 'D'
        else:  # æ›´é•¿æ—¶é—´
            # æ¯æœˆä¸€ä¸ªåˆ»åº¦ï¼Œé‡ç‚¹åŒºåŸŸæ¯å‘¨
            base_freq = 'M'
            dense_freq = 'W'

        # ç”ŸæˆåŸºç¡€åˆ»åº¦
        base_ticks = pd.date_range(start=start_time, end=end_time, freq=base_freq)

        # åœ¨å…³é”®åŒºåŸŸæ·»åŠ å¯†é›†åˆ»åº¦
        dense_ticks = []

        # é¢„æµ‹å¼€å§‹æ—¶é—´å‰åçš„å¯†é›†åˆ»åº¦
        pred_dense_start = pred_start - pd.Timedelta(hours=min(total_hours * 0.1, 24))
        pred_dense_end = pred_start + pd.Timedelta(hours=min(total_hours * 0.1, 24))
        if pred_dense_start < end_time and pred_dense_end > start_time:
            pred_dense = pd.date_range(
                start=max(pred_dense_start, start_time),
                end=min(pred_dense_end, end_time),
                freq=dense_freq
            )
            dense_ticks.extend(pred_dense)

        # å½“å‰æ—¶é—´å‰åçš„å¯†é›†åˆ»åº¦ï¼ˆå¦‚æœæ˜¯æœªæ¥é¢„æµ‹ï¼‰
        if is_future_forecast and abs(current_time - end_time) < pd.Timedelta(days=30):
            current_dense_start = current_time - pd.Timedelta(hours=min(total_hours * 0.15, 48))
            current_dense_end = min(current_time + pd.Timedelta(hours=min(total_hours * 0.15, 48)), end_time)
            if current_dense_start < end_time and current_dense_end > start_time:
                current_dense = pd.date_range(
                    start=max(current_dense_start, start_time),
                    end=current_dense_end,
                    freq=dense_freq
                )
                dense_ticks.extend(current_dense)

        # åˆå¹¶æ‰€æœ‰åˆ»åº¦å¹¶å»é‡
        all_ticks = sorted(set(base_ticks).union(set(dense_ticks)))
        all_ticks = [t for t in all_ticks if start_time <= t <= end_time]

        # é™åˆ¶åˆ»åº¦æ•°é‡
        if len(all_ticks) > max_ticks:
            # ä¼˜å…ˆä¿ç•™å…³é”®åŒºåŸŸçš„åˆ»åº¦
            key_ticks = []
            other_ticks = []

            for tick in all_ticks:
                if (abs(tick - pred_start) < pd.Timedelta(hours=24) or
                    (is_future_forecast and abs(tick - current_time) < pd.Timedelta(hours=24))):
                    key_ticks.append(tick)
                else:
                    other_ticks.append(tick)

            # ä¿ç•™æ‰€æœ‰å…³é”®åˆ»åº¦ï¼Œç„¶åå‡åŒ€é‡‡æ ·å…¶ä»–åˆ»åº¦
            remaining_slots = max_ticks - len(key_ticks)
            if remaining_slots > 0 and other_ticks:
                step = max(1, len(other_ticks) // remaining_slots)
                sampled_other = other_ticks[::step][:remaining_slots]
                all_ticks = sorted(set(key_ticks + sampled_other))
            else:
                all_ticks = sorted(key_ticks)

        # ç”Ÿæˆåˆ»åº¦æ ‡ç­¾
        tick_labels = []
        for tick in all_ticks:
            if total_hours <= 24:  # 1å¤©å†…æ˜¾ç¤ºæ—¶åˆ†
                tick_labels.append(tick.strftime('%m-%d %H:%M'))
            elif total_hours <= 168:  # 1å‘¨å†…æ˜¾ç¤ºæ—¥æœŸå’Œå°æ—¶
                tick_labels.append(tick.strftime('%m-%d %H:00'))
            else:  # æ›´é•¿æ—¶é—´æ˜¾ç¤ºæ—¥æœŸ
                tick_labels.append(tick.strftime('%m-%d'))

        return all_ticks, tick_labels
    
    def plot_prediction(self, historical_df, pred_df, symbol, is_future_forecast=False, save_plot=True, plot_lookback=1500):
        """
        ç»˜åˆ¶é¢„æµ‹ç»“æœ - æ™ºèƒ½æ—¶é—´è½´æ˜¾ç¤ºï¼ŒåŒºåˆ†é¢„æµ‹å’Œå›æµ‹æ¨¡å¼
        """
        try:
            logger.info("å¼€å§‹ç»˜åˆ¶é¢„æµ‹å›¾è¡¨...")
            logger.info(f"è¾“å…¥å‚æ•°: symbol={symbol}, is_future_forecast={is_future_forecast}, save_plot={save_plot}, plot_lookback={plot_lookback}")
            logger.info(f"å†å²æ•°æ®å½¢çŠ¶: {historical_df.shape if historical_df is not None else 'None'}")
            logger.info(f"é¢„æµ‹æ•°æ®å½¢çŠ¶: {pred_df.shape if pred_df is not None else 'None'}")

            # éªŒè¯è¾“å…¥æ•°æ®
            if pred_df is None or pred_df.empty:
                logger.error("é¢„æµ‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»˜åˆ¶å›¾è¡¨")
                return None

            if historical_df is None or historical_df.empty:
                logger.error("å†å²æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»˜åˆ¶å›¾è¡¨")
                return None

            # è®¾ç½®matplotlibåç«¯ä¸ºéäº¤äº’å¼
            logger.info("é…ç½®matplotlib...")
            import matplotlib
            matplotlib.use('Agg')  # ç¡®ä¿ä½¿ç”¨éäº¤äº’å¼åç«¯
            import matplotlib.pyplot as plt
            logger.info(f"matplotlibåç«¯: {matplotlib.get_backend()}")

            # è®¾ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False

            logger.info("matplotlibé…ç½®å®Œæˆ")
            
            # å‡†å¤‡ç»˜å›¾æ•°æ®
            start_pred_time = pred_df.index.min()
            logger.info(f"é¢„æµ‹å¼€å§‹æ—¶é—´: {start_pred_time}, ç±»å‹: {type(start_pred_time)}")

            # æ£€æŸ¥æ•°æ®ç±»å‹
            logger.info(f"å†å²æ•°æ®timestampsç±»å‹: {historical_df['timestamps'].dtype}")
            logger.info(f"é¢„æµ‹æ•°æ®ç´¢å¼•ç±»å‹: {pred_df.index.dtype}")

            # ç¡®ä¿æ—¶é—´æˆ³æ ¼å¼ä¸€è‡´å¹¶ç­›é€‰æ•°æ®
            try:
                hist_timestamps = pd.to_datetime(historical_df['timestamps'])
                pred_start = pd.to_datetime(start_pred_time)
                historical_plot_df = historical_df[hist_timestamps < pred_start].tail(plot_lookback)
                logger.info(f"å†å²æ•°æ®ç‚¹æ•°: {len(historical_plot_df)}")
            except Exception as e:
                logger.error(f"æ—¶é—´æˆ³å¤„ç†å¤±è´¥: {e}")
                return None

            # åˆå¹¶æ‰€æœ‰æ—¶é—´ç‚¹ç”¨äºæ™ºèƒ½åˆ»åº¦è®¡ç®—
            timestamps_list = []
            if not historical_plot_df.empty:
                timestamps_list.append(historical_plot_df['timestamps'])
            timestamps_list.append(pd.Series(pred_df.index))

            all_timestamps = pd.concat(timestamps_list).sort_values().drop_duplicates()
            logger.info(f"æ€»æ—¶é—´ç‚¹æ•°: {len(all_timestamps)}")

            # åˆ›å»ºå›¾è¡¨
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8), sharex=True)
            logger.info("matplotlibå›¾å½¢åˆ›å»ºå®Œæˆ")
            
            # --- ä»·æ ¼å›¾ ---
            ax1.plot(historical_plot_df['timestamps'], historical_plot_df['close'], label='å†å²ä»·æ ¼', color='blue', linewidth=1.5)
            ax1.plot(pred_df.index, pred_df['close'], label='é¢„æµ‹ä»·æ ¼', color='red', linewidth=2, linestyle='--')
            
            # åœ¨å›æµ‹æ¨¡å¼ä¸‹ï¼Œæ·»åŠ çœŸå®ä»·æ ¼æ›²çº¿
            if not is_future_forecast:
                true_values_df = historical_df[historical_df['timestamps'].isin(pred_df.index)]
                ax1.plot(true_values_df['timestamps'], true_values_df['close'], label='çœŸå®ä»·æ ¼', color='green', linewidth=2, alpha=0.8)

            # æ·»åŠ å…³é”®æ—¶é—´åŒºåŸŸæ ‡è®°
            current_time = pd.Timestamp.now()

            # æ ‡è®°é¢„æµ‹å¼€å§‹æ—¶é—´
            pred_start_time = pred_df.index.min()
            ax1.axvline(pred_start_time, color='orange', linestyle=':', linewidth=1.5, alpha=0.7)
            ax1.text(pred_start_time, ax1.get_ylim()[1]*0.95, 'é¢„æµ‹å¼€å§‹',
                    ha='center', va='top', fontsize=10,
                    bbox=dict(boxstyle='round,pad=0.3', facecolor='orange', alpha=0.7))

            # æ ‡è®°å½“å‰æ—¶é—´ï¼ˆå¦‚æœæ˜¯æœªæ¥é¢„æµ‹ä¸”æ¥è¿‘å½“å‰æ—¶é—´ï¼‰
            if is_future_forecast and abs(current_time - all_timestamps.max()) < pd.Timedelta(days=7):
                ax1.axvline(current_time, color='purple', linestyle=':', linewidth=1.5, alpha=0.7)
                ax1.text(current_time, ax1.get_ylim()[1]*0.90, 'å½“å‰æ—¶é—´',
                        ha='center', va='top', fontsize=10,
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='purple', alpha=0.7))

            # æ ‡è®°å†å²æ•°æ®ç»“æŸæ—¶é—´
            if not historical_plot_df.empty:
                hist_end_time = historical_plot_df['timestamps'].iloc[-1]
                ax1.axvline(hist_end_time, color='gray', linestyle='--', linewidth=1, alpha=0.8)
                ax1.text(hist_end_time, ax1.get_ylim()[0]*1.05, 'å†å²ç»“æŸ',
                        ha='center', va='bottom', fontsize=9,
                        bbox=dict(boxstyle='round,pad=0.2', facecolor='gray', alpha=0.6))
            
            ax1.set_ylabel('ä»·æ ¼', fontsize=14)
            mode_name = 'æœªæ¥é¢„æµ‹' if is_future_forecast else 'å†å²å›æµ‹'
            mode_color = 'orange' if is_future_forecast else 'blue'
            mode_desc = 'é¢„æµ‹æœªæ¥è¶‹åŠ¿' if is_future_forecast else 'éªŒè¯å†å²è¡¨ç°'

            ax1.set_title(f'{symbol} è‚¡ç¥¨{mode_name}ç»“æœ - æ™ºèƒ½æ—¶é—´è½´', fontsize=16)

            # åœ¨å›¾è¡¨å·¦ä¸Šè§’æ·»åŠ æ¨¡å¼æ ‡è¯†æ¡†
            ax1.text(0.02, 0.98, f'{mode_name}\n{mode_desc}',
                    transform=ax1.transAxes, fontsize=11, verticalalignment='top',
                    bbox=dict(boxstyle='round,pad=0.5', facecolor=mode_color, alpha=0.8),
                    color='white', fontweight='bold')

            ax1.legend(loc='upper left', fontsize=12)
            ax1.grid(True, alpha=0.3)
            
            # --- æˆäº¤é‡å›¾ ---
            ax2.plot(historical_plot_df['timestamps'], historical_plot_df['volume'], label='å†å²æˆäº¤é‡', color='blue', linewidth=1.5)
            ax2.plot(pred_df.index, pred_df['volume'], label='é¢„æµ‹æˆäº¤é‡', color='red', linewidth=2, linestyle='--')
            
            if not is_future_forecast:
                true_values_df = historical_df[historical_df['timestamps'].isin(pred_df.index)]
                ax2.plot(true_values_df['timestamps'], true_values_df['volume'], label='çœŸå®æˆäº¤é‡', color='green', linewidth=2, alpha=0.8)

            # åœ¨æˆäº¤é‡å›¾ä¸Šä¹Ÿæ·»åŠ å…³é”®æ—¶é—´æ ‡è®°
            ax2.axvline(pred_start_time, color='orange', linestyle=':', linewidth=1.5, alpha=0.7)

            if is_future_forecast and abs(current_time - all_timestamps.max()) < pd.Timedelta(days=7):
                ax2.axvline(current_time, color='purple', linestyle=':', linewidth=1.5, alpha=0.7)

            if not historical_plot_df.empty:
                hist_end_time = historical_plot_df['timestamps'].iloc[-1]
                ax2.axvline(hist_end_time, color='gray', linestyle='--', linewidth=1, alpha=0.8)
            
            ax2.set_ylabel('æˆäº¤é‡', fontsize=14)
            ax2.set_xlabel('æ—¶é—´', fontsize=14)
            ax2.legend(loc='upper left', fontsize=12)
            ax2.grid(True, alpha=0.3)

            # è®¾ç½®æ™ºèƒ½æ—¶é—´è½´åˆ»åº¦
            smart_ticks, tick_labels = self._calculate_smart_xticks(all_timestamps, max_ticks=20, is_future_forecast=is_future_forecast)
            if smart_ticks:
                ax2.set_xticks(smart_ticks)
                ax2.set_xticklabels(tick_labels, rotation=45, ha='right', fontsize=10)

            # æ·»åŠ å±€éƒ¨æ”¾å¤§è§†å›¾ï¼ˆå¦‚æœæ—¶é—´è·¨åº¦è¾ƒå¤§ä¸”é¢„æµ‹åŒºåŸŸè¾ƒçŸ­ï¼‰
            total_duration = all_timestamps.max() - all_timestamps.min()
            pred_duration = pred_df.index.max() - pred_df.index.min()

            if total_duration > pd.Timedelta(days=30) and pred_duration < total_duration * 0.3:
                # åˆ›å»ºå±€éƒ¨æ”¾å¤§å­å›¾
                # è®¡ç®—é¢„æµ‹åŒºåŸŸçš„æ‰©å±•èŒƒå›´ï¼ˆå‰åå„å»¶é•¿10%ï¼‰
                pred_range = pred_df.index.max() - pred_df.index.min()
                zoom_margin = pred_range * 0.2

                zoom_start = max(all_timestamps.min(), pred_df.index.min() - zoom_margin)
                zoom_end = min(all_timestamps.max(), pred_df.index.max() + zoom_margin)

                # åˆ›å»ºå­å›¾ä½ç½®
                from mpl_toolkits.axes_grid1.inset_locator import inset_axes
                ax_zoom = inset_axes(ax1, width="35%", height="25%", loc='upper right',
                                    bbox_to_anchor=(0.02, 0.02, 0.96, 0.96),
                                    bbox_transform=ax1.transAxes)

                # åœ¨å­å›¾ä¸­ç»˜åˆ¶å±€éƒ¨æ”¾å¤§çš„ä»·æ ¼æ•°æ®
                zoom_hist = historical_plot_df[
                    (historical_plot_df['timestamps'] >= zoom_start) &
                    (historical_plot_df['timestamps'] <= zoom_end)
                ]
                zoom_pred = pred_df[
                    (pred_df.index >= zoom_start) &
                    (pred_df.index <= zoom_end)
                ]

                if not zoom_hist.empty:
                    ax_zoom.plot(zoom_hist['timestamps'], zoom_hist['close'],
                               color='blue', linewidth=1, alpha=0.7)
                if not zoom_pred.empty:
                    ax_zoom.plot(zoom_pred.index, zoom_pred['close'],
                               color='red', linewidth=1.5, linestyle='--', alpha=0.8)

                # å­å›¾æ ·å¼è®¾ç½®
                ax_zoom.set_title('é¢„æµ‹åŒºåŸŸæ”¾å¤§', fontsize=9, pad=2)
                ax_zoom.tick_params(labelsize=8)
                ax_zoom.grid(True, alpha=0.3)

                # æ·»åŠ è¾¹æ¡†
                for spine in ax_zoom.spines.values():
                    spine.set_edgecolor('orange')
                    spine.set_linewidth(1)

            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            plot_path = None
            if save_plot:
                try:
                    # --- åˆ›å»ºæ¨¡å¼åŒºåˆ†çš„ç»“æœå­æ–‡ä»¶å¤¹ ---
                    mode_dir = 'future_forecast' if is_future_forecast else 'backtest'
                    symbol_results_dir = os.path.join(self.results_dir, symbol, mode_dir)
                    os.makedirs(symbol_results_dir, exist_ok=True)
                    logger.info(f"åˆ›å»ºç»“æœç›®å½•: {symbol_results_dir}")

                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    mode_prefix = 'forecast' if is_future_forecast else 'backtest'
                    plot_filename = f"{symbol}_{mode_prefix}_chart_{timestamp}.png"
                    plot_path = os.path.join(symbol_results_dir, plot_filename)

                    logger.info(f"æ­£åœ¨ä¿å­˜å›¾è¡¨åˆ°: {plot_path}")
                    plt.savefig(plot_path, dpi=300, bbox_inches='tight')

                    # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸä¿å­˜
                    if os.path.exists(plot_path):
                        file_size = os.path.getsize(plot_path)
                        logger.info(f"âœ… å›¾è¡¨ä¿å­˜æˆåŠŸ: {plot_path} ({file_size} bytes)")
                    else:
                        logger.error(f"âŒ å›¾è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {plot_path}")
                        plot_path = None

                except Exception as save_error:
                    logger.error(f"ä¿å­˜å›¾è¡¨å¤±è´¥: {str(save_error)}")
                    plot_path = None

            # åœ¨éäº¤äº’ç¯å¢ƒä¸­ä¸æ˜¾ç¤ºå›¾è¡¨ï¼Œé¿å…é˜»å¡
            # æ³¨æ„ï¼šæˆ‘ä»¬å·²ç»è®¾ç½®äº†Aggåç«¯ï¼Œæ‰€ä»¥plt.show()ä¸ä¼šå®é™…æ˜¾ç¤ºå›¾åƒ
            try:
                plt.show()  # åœ¨Aggåç«¯ä¸‹è¿™ä¸ä¼šæ˜¾ç¤ºä»»ä½•ä¸œè¥¿ï¼Œåªæ˜¯ä¸ºäº†å…¼å®¹æ€§
                logger.info("å›¾è¡¨æ˜¾ç¤ºè°ƒç”¨å®Œæˆ")
            except Exception as show_error:
                logger.warning(f"å›¾è¡¨æ˜¾ç¤ºæ—¶å‡ºç°è­¦å‘Š: {str(show_error)}")
            
            logger.info("å›¾è¡¨ç»˜åˆ¶æµç¨‹å®Œæˆ")
            return plot_path
            
        except Exception as e:
            logger.error(f"ç»˜åˆ¶å›¾è¡¨å¤±è´¥: {str(e)}")
            logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", exc_info=True)
            return None
    
    def save_prediction_results(self, pred_df, symbol, metadata=None, is_future_forecast=False):
        """
        ä¿å­˜é¢„æµ‹ç»“æœ
        
        Args:
            pred_df (pd.DataFrame): é¢„æµ‹ç»“æœ
            symbol (str): è‚¡ç¥¨ä»£ç 
            metadata (dict): å…ƒæ•°æ®
            
        Returns:
            str: ç»“æœæ–‡ä»¶è·¯å¾„
        """
        try:
            # --- åˆ›å»ºæ¨¡å¼åŒºåˆ†çš„ç»“æœå­æ–‡ä»¶å¤¹ ---
            mode_dir = 'future_forecast' if is_future_forecast else 'backtest'
            symbol_results_dir = os.path.join(self.results_dir, symbol, mode_dir)
            os.makedirs(symbol_results_dir, exist_ok=True)

            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            mode_prefix = 'forecast' if is_future_forecast else 'backtest'

            # ä¿å­˜é¢„æµ‹æ•°æ®
            csv_filename = f"{symbol}_{mode_prefix}_data_{timestamp}.csv"
            csv_path = os.path.join(symbol_results_dir, csv_filename)
            
            # å°†ç´¢å¼•é‡ç½®ä¸ºåˆ—ï¼Œå¹¶ç¡®ä¿åˆ—åä¸º'timestamps'
            save_df = pred_df.reset_index()
            if 'index' in save_df.columns:
                save_df = save_df.rename(columns={'index': 'timestamps'})
            
            save_df.to_csv(csv_path, index=False)
            
            # ä¿å­˜å…ƒæ•°æ®
            if metadata is None:
                metadata = {}
            
            metadata.update({
                'symbol': symbol,
                'prediction_time': timestamp,
                'data_points': len(pred_df),
                'columns': list(pred_df.columns)
            })
            
            json_filename = f"{symbol}_{mode_prefix}_metadata_{timestamp}.json"
            json_path = os.path.join(symbol_results_dir, json_filename)
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2, default=str)
            
            mode_desc = "æœªæ¥é¢„æµ‹" if is_future_forecast else "å†å²å›æµ‹"
            print(f"âœ… {mode_desc}ç»“æœå·²ä¿å­˜è‡³ä¸“é—¨æ–‡ä»¶å¤¹:")
            print(f"   ğŸ“‚ ç›®å½•: {symbol_results_dir}")
            print(f"   ğŸ“„ æ•°æ®æ–‡ä»¶: {os.path.basename(csv_path)}")
            print(f"   ğŸ“‹ å…ƒæ•°æ®æ–‡ä»¶: {os.path.basename(json_path)}")
            print(f"   ğŸ¯ æ¨¡å¼: {mode_desc}")
            
            return csv_path
            
        except Exception as e:
            print(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return None
    
    def analyze_prediction(self, historical_df, pred_df, symbol, is_future_forecast):
        """
        å…¨é¢åˆ†æé¢„æµ‹ç»“æœ
        """
        try:
            logger.info("å¼€å§‹è¯¦ç»†åˆ†æé¢„æµ‹ç»“æœ...")

            # è·å–å†å²æ•°æ®çš„æœ€åä¸€ä¸ªç‚¹ç”¨äºæ¯”è¾ƒ
            last_historical_point = historical_df.iloc[-1]
            last_close = last_historical_point['close']
            
            # === 1. åŸºç¡€ç»Ÿè®¡åˆ†æ ===
            pred_close_stats = self._calculate_price_statistics(pred_df)
            pred_volume_stats = self._calculate_volume_statistics(pred_df)

            # === 2. è¶‹åŠ¿å’Œå˜åŒ–åˆ†æ ===
            trend_analysis = self._analyze_trend_and_changes(historical_df, pred_df, is_future_forecast)

            # === 3. é£é™©å’Œæ³¢åŠ¨æ€§åˆ†æ ===
            risk_analysis = self._analyze_risk_and_volatility(historical_df, pred_df)

            # === 4. é¢„æµ‹è´¨é‡è¯„ä¼° ===
            quality_metrics = self._evaluate_prediction_quality(historical_df, pred_df, is_future_forecast)

            # === 5. æŠ€æœ¯æŒ‡æ ‡åˆ†æ ===
            technical_analysis = self._calculate_technical_indicators(pred_df)

            # æ•´åˆæ‰€æœ‰åˆ†æç»“æœ
            analysis = {
                'symbol': symbol,
                'historical_last_close': last_close,
                'prediction_period_days': len(pred_df),
                'is_future_forecast': is_future_forecast,
                'price_analysis': {
                    **pred_close_stats,
                    **trend_analysis['price']
                },
                'volume_analysis': pred_volume_stats,
                'risk_analysis': risk_analysis,
                'quality_metrics': quality_metrics,
                'technical_analysis': technical_analysis,
                'timestamp': datetime.now().isoformat()
            }

            logger.info(f"é¢„æµ‹åˆ†æå®Œæˆ - è¶‹åŠ¿: {analysis['price_analysis']['trend']}, "
                       f"é¢„æœŸå˜åŒ–: {analysis['price_analysis']['price_change_percentage']:.2f}%")
            return analysis

        except Exception as e:
            logger.error(f"é¢„æµ‹ç»“æœåˆ†æå¤±è´¥: {str(e)}")
            return None

    def _calculate_price_statistics(self, pred_df):
        """è®¡ç®—ä»·æ ¼åŸºç¡€ç»Ÿè®¡"""
        close_prices = pred_df['close']

        return {
            'mean': close_prices.mean(),
            'std': close_prices.std(),
            'min': close_prices.min(),
            'max': close_prices.max(),
            'median': close_prices.median(),
            'q25': close_prices.quantile(0.25),
            'q75': close_prices.quantile(0.75),
            'range': close_prices.max() - close_prices.min(),
            'cv': close_prices.std() / close_prices.mean() if close_prices.mean() != 0 else 0
        }

    def _calculate_volume_statistics(self, pred_df):
        """è®¡ç®—æˆäº¤é‡ç»Ÿè®¡"""
        if 'volume' not in pred_df.columns:
            return {'trend': 'æ— æ•°æ®', 'avg_volume': 0}

        volumes = pred_df['volume']

        return {
            'mean': volumes.mean(),
            'std': volumes.std(),
            'min': volumes.min(),
            'max': volumes.max(),
            'trend': 'å¢åŠ ' if volumes.iloc[-1] > volumes.iloc[0] else 'å‡å°‘',
            'volatility': volumes.std() / volumes.mean() if volumes.mean() != 0 else 0
        }

    def _analyze_trend_and_changes(self, historical_df, pred_df, is_future_forecast):
        """åˆ†æè¶‹åŠ¿å’Œå˜åŒ–"""
        # ç¡®å®šæ¯”è¾ƒåŸºå‡†ç‚¹
        if is_future_forecast:
            baseline_close = historical_df.iloc[-1]['close']
        else:
            comparison_point = historical_df[historical_df['timestamps'] < pred_df.index.min()]
            baseline_close = comparison_point.iloc[-1]['close'] if not comparison_point.empty else historical_df.iloc[-1]['close']

        # è®¡ç®—é¢„æµ‹çš„å˜åŒ–
        pred_start = pred_df['close'].iloc[0]
        pred_end = pred_df['close'].iloc[-1]

        price_change = pred_end - baseline_close
        price_change_pct = (price_change / baseline_close) * 100 if baseline_close != 0 else 0

        # è¶‹åŠ¿å¼ºåº¦åˆ†æ
        pred_trend_strength = abs(pred_end - pred_start) / pred_start if pred_start != 0 else 0

        return {
            'price': {
                'baseline_close': baseline_close,
                'pred_start': pred_start,
                'pred_end': pred_end,
                    'price_change': price_change,
                    'price_change_percentage': price_change_pct,
                'trend': 'ä¸Šæ¶¨' if pred_end > pred_start else 'ä¸‹è·Œ',
                'trend_strength': pred_trend_strength,
                'direction_consistency': 'ä¸€è‡´' if (pred_end > pred_start) == (price_change > 0) else 'ä¸ä¸€è‡´'
            }
        }

    def _analyze_risk_and_volatility(self, historical_df, pred_df):
        """åˆ†æé£é™©å’Œæ³¢åŠ¨æ€§"""
        # è®¡ç®—é¢„æµ‹æ•°æ®çš„æ³¢åŠ¨æ€§
        pred_returns = pred_df['close'].pct_change().dropna()
        pred_volatility = pred_returns.std() * (252 ** 0.5)  # å¹´åŒ–æ³¢åŠ¨ç‡

        # è®¡ç®—å†å²æ•°æ®çš„æ³¢åŠ¨æ€§ä½œä¸ºå¯¹æ¯”
        hist_returns = historical_df['close'].pct_change().dropna().tail(100)  # æœ€è¿‘100ä¸ªç‚¹
        hist_volatility = hist_returns.std() * (252 ** 0.5) if len(hist_returns) > 0 else 0

        # é£é™©æŒ‡æ ‡
        var_95 = np.percentile(pred_returns, 5)  # 95% VaR
        max_drawdown = self._calculate_max_drawdown(pred_df['close'])

        return {
            'pred_volatility': pred_volatility,
            'hist_volatility': hist_volatility,
            'volatility_ratio': pred_volatility / hist_volatility if hist_volatility != 0 else float('inf'),
            'var_95': var_95,
            'max_drawdown': max_drawdown,
            'sharpe_ratio': pred_returns.mean() / pred_returns.std() if pred_returns.std() != 0 else 0
        }

    def _evaluate_prediction_quality(self, historical_df, pred_df, is_future_forecast):
        """è¯„ä¼°é¢„æµ‹è´¨é‡"""
        quality_scores = {}

        # 1. å¹³æ»‘æ€§è¯„åˆ† (0-1, 1è¡¨ç¤ºéå¸¸å¹³æ»‘)
        returns = pred_df['close'].pct_change().dropna()
        smoothness = 1 / (1 + returns.std())  # æ³¢åŠ¨ç‡è¶Šä½ï¼Œå¹³æ»‘æ€§è¶Šé«˜
        quality_scores['smoothness'] = smoothness

        # 2. åˆç†æ€§è¯„åˆ† (åŸºäºå†å²æ³¢åŠ¨æ€§)
        hist_volatility = historical_df['close'].pct_change().dropna().tail(50).std()
        pred_volatility = returns.std()

        if hist_volatility > 0:
            volatility_ratio = pred_volatility / hist_volatility
            # ç†æƒ³çš„æ³¢åŠ¨ç‡æ¯”åº”è¯¥åœ¨0.5-2.0ä¹‹é—´
            if 0.5 <= volatility_ratio <= 2.0:
                reasonableness = 1.0
            else:
                reasonableness = max(0, 1 - abs(np.log(volatility_ratio)))
        else:
            reasonableness = 0.5

        quality_scores['reasonableness'] = reasonableness

        # 3. è¿ç»­æ€§è¯„åˆ† (æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸è·³è·ƒ)
        jumps = (returns.abs() > 0.1).sum()  # å•æ­¥å˜åŒ–è¶…è¿‡10%çš„æ¬¡æ•°
        continuity = max(0, 1 - jumps / len(returns))
        quality_scores['continuity'] = continuity

        # 4. æ•´ä½“è´¨é‡è¯„åˆ†
        quality_scores['overall_score'] = np.mean([smoothness, reasonableness, continuity])

        return quality_scores

    def _calculate_technical_indicators(self, pred_df):
        """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡"""
        indicators = {}

        if len(pred_df) < 14:  # æ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            return indicators

        close_prices = pred_df['close']

        # ç®€å•ç§»åŠ¨å¹³å‡
        indicators['sma_5'] = close_prices.rolling(5).mean().iloc[-1] if len(close_prices) >= 5 else None
        indicators['sma_10'] = close_prices.rolling(10).mean().iloc[-1] if len(close_prices) >= 10 else None

        # RSI (ç®€åŒ–ç‰ˆ)
        if len(close_prices) >= 14:
            delta = close_prices.diff()
            gain = (delta.where(delta > 0, 0)).rolling(14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
            rs = gain / loss
            indicators['rsi_14'] = 100 - (100 / (1 + rs)).iloc[-1]
        else:
            indicators['rsi_14'] = None

        return indicators

    def _calculate_max_drawdown(self, prices):
        """è®¡ç®—æœ€å¤§å›æ’¤"""
        peak = prices.expanding().max()
        drawdown = (prices - peak) / peak
        return drawdown.min()
    
    def run_prediction_pipeline(self, historical_df, x_df, x_timestamp, y_timestamp,
                               is_future_forecast, symbol, pred_len,
                               T=1.0, top_p=0.9, sample_count=1, plot_lookback=1500):
        """
        è¿è¡Œå®Œæ•´çš„é¢„æµ‹æµç¨‹
        """
        mode_name = "æœªæ¥é¢„æµ‹" if is_future_forecast else "å†å²å›æµ‹"
        logger.info(f"ğŸš€ å¼€å§‹ {symbol} çš„{mode_name}æµç¨‹...")

        try:
            # === æ•°æ®éªŒè¯å’Œé¢„å¤„ç† ===
            logger.info("ğŸ“Š æ•°æ®éªŒè¯å’Œé¢„å¤„ç†...")

            # éªŒè¯å†å²æ•°æ®
            is_valid, error_msg = self.validate_data(historical_df, "historical_data")
            if not is_valid:
                logger.error(f"å†å²æ•°æ®éªŒè¯å¤±è´¥: {error_msg}")
                return None

            # éªŒè¯è¾“å…¥æ•°æ®
            input_df = x_df.copy()
            input_df[self.data_config['timestamp_column']] = x_timestamp
            is_valid, error_msg = self.validate_data(input_df, "input_data")
            if not is_valid:
                logger.error(f"è¾“å…¥æ•°æ®éªŒè¯å¤±è´¥: {error_msg}")
                return None

            # é¢„å¤„ç†å†å²æ•°æ®
            historical_df = self.preprocess_data(historical_df)
            x_df = self.preprocess_data(input_df)[self.data_config['required_columns']]

            # ç¡®ä¿ y_timestamp æ˜¯æ­£ç¡®çš„ç±»å‹
            y_timestamp_series = pd.Series(pd.to_datetime(y_timestamp))

            # === 1. è¿›è¡Œé¢„æµ‹ ===
            logger.info("ğŸ”® å¼€å§‹æ¨¡å‹æ¨ç†...")
            pred_df = self.predict(x_df, x_timestamp, y_timestamp_series, pred_len, T, top_p, sample_count)
            if pred_df is None:
                logger.error("é¢„æµ‹å¤±è´¥ï¼Œè¿”å›None")
                return None

            # === 2. ç¡®ä¿ pred_df çš„ç´¢å¼•æ˜¯ DatetimeIndex ===
            pred_df.index = pd.to_datetime(pred_df.index)

            # === 3. åˆ†æé¢„æµ‹ç»“æœ ===
            logger.info("ğŸ“ˆ åˆ†æé¢„æµ‹ç»“æœ...")
            analysis = self.analyze_prediction(historical_df, pred_df, symbol, is_future_forecast)
            if analysis is None:
                logger.error("é¢„æµ‹ç»“æœåˆ†æå¤±è´¥")
                return None

            # === 4. ç»˜åˆ¶å›¾è¡¨ ===
            logger.info("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
            plot_path = self.plot_prediction(historical_df, pred_df, symbol, is_future_forecast, plot_lookback=plot_lookback)
            if plot_path is None:
                logger.error("âŒ å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼Œplot_pathä¸ºNone")
            else:
                logger.info(f"âœ… å›¾è¡¨ç”ŸæˆæˆåŠŸ: {plot_path}")

            # === 5. ä¿å­˜ç»“æœ ===
            logger.info("ğŸ’¾ ä¿å­˜é¢„æµ‹ç»“æœ...")
            metadata = {
                'analysis': analysis,
                'plot_path': plot_path,
                'parameters': {
                    'pred_len': pred_len,
                    'T': T,
                    'top_p': top_p,
                    'sample_count': sample_count,
                    'is_future_forecast': is_future_forecast
                },
                'data_quality': {
                    'input_points': len(x_df),
                    'prediction_points': len(pred_df),
                    'processing_timestamp': datetime.now().isoformat()
                }
            }

            csv_path = self.save_prediction_results(pred_df, symbol, metadata, is_future_forecast)

            # === 6. è¿”å›å®Œæ•´ç»“æœ ===
            results = {
                'symbol': symbol,
                'prediction': pred_df,
                'analysis': analysis,
                'files': {
                    'csv_path': csv_path,
                    'plot_path': plot_path
                },
                'metadata': metadata
            }

            logger.info(f"ğŸ‰ {mode_name}æµç¨‹å…¨éƒ¨å®Œæˆï¼")
            logger.info(f"ğŸ“ˆ {mode_name}å›¾è¡¨å·²ä¿å­˜è‡³: {plot_path}")
            logger.info(f"ğŸ“„ {mode_name}æ•°æ®å·²ä¿å­˜è‡³: {csv_path}")

            return results

        except Exception as e:
            logger.error(f"é¢„æµ‹æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}")
            logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", exc_info=True)
            return None

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="Kronos è‚¡ç¥¨é¢„æµ‹å™¨ - ç‹¬ç«‹è¿è¡Œç‰ˆæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python stock_predictor.py                                    # ä½¿ç”¨é»˜è®¤è®¾ç½®è¿è¡Œç¤ºä¾‹
  python stock_predictor.py --device cpu                      # ä½¿ç”¨ CPU è¿è¡Œ
  python stock_predictor.py --device cuda:0                   # ä½¿ç”¨ GPU è¿è¡Œ
  python stock_predictor.py --data-path /path/to/data.csv --symbol 000001  # ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®

å‚æ•°è¯´æ˜:
  device: è®¡ç®—è®¾å¤‡é€‰æ‹©
    - auto: è‡ªåŠ¨æ£€æµ‹ (é»˜è®¤ï¼Œæ¨è)
    - cpu: ä½¿ç”¨ CPU
    - cuda:0: ä½¿ç”¨ç¬¬ä¸€ä¸ª CUDA GPU
    - mps: ä½¿ç”¨ Apple Silicon GPU

  data-path: è‡ªå®šä¹‰æ•°æ®æ–‡ä»¶è·¯å¾„ (å¯é€‰)
    å¦‚æœä¸æŒ‡å®šï¼Œå°†ä½¿ç”¨é¡¹ç›®ä¸­çš„ç¤ºä¾‹æ•°æ®

  symbol: è‚¡ç¥¨ä»£ç  (å¯é€‰)
    ä¸ data-path é…åˆä½¿ç”¨ï¼Œé»˜è®¤ '600977'
        """
    )

    parser.add_argument(
        "--device", "-d",
        default="auto",
        choices=["auto", "cpu", "cuda", "cuda:0", "mps"],
        help="è®¡ç®—è®¾å¤‡ (é»˜è®¤: auto)"
    )

    parser.add_argument(
        "--data-path",
        help="è‡ªå®šä¹‰æ•°æ®æ–‡ä»¶è·¯å¾„ (CSVæ ¼å¼ï¼ŒåŒ…å«OHLCVæ•°æ®)"
    )

    parser.add_argument(
        "--symbol", "-s",
        default="600977",
        help="è‚¡ç¥¨ä»£ç  (é»˜è®¤: 600977)"
    )

    parser.add_argument(
        "--lookback", "-l",
        type=int,
        default=1500,
        help="å†å²æ•°æ®ç‚¹æ•°é‡ (é»˜è®¤: 1500)"
    )

    parser.add_argument(
        "--pred-len", "-p",
        type=int,
        default=96,
        help="é¢„æµ‹æ•°æ®ç‚¹æ•°é‡ (é»˜è®¤: 96)"
    )

    parser.add_argument(
        "--future-forecast",
        action="store_true",
        help="æœªæ¥é¢„æµ‹æ¨¡å¼ (é»˜è®¤: Falseï¼Œå›æµ‹æ¨¡å¼)"
    )

    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    print("="*60)
    print("ğŸ¯ Kronos è‚¡ç¥¨é¢„æµ‹å™¨ - ç‹¬ç«‹è¿è¡Œç‰ˆæœ¬")
    print("="*60)
    print(f"ğŸ“‹ ä½¿ç”¨å‚æ•°: device={args.device}, symbol={args.symbol}")

    # 1. åˆ›å»ºé¢„æµ‹å™¨
    try:
        print(f"\nğŸš€ æ­£åœ¨åˆå§‹åŒ–é¢„æµ‹å™¨ (device: {args.device})...")
        predictor = StockPredictor(device=args.device)
        print("âœ… é¢„æµ‹å™¨åˆå§‹åŒ–æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ é¢„æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        print("\nğŸ”§ å¯èƒ½çš„åŸå› :")
        print("  1. ç¼ºå°‘ä¾èµ–åŒ…ï¼Œè¯·è¿è¡Œ: pip install -r requirements.txt")
        print("  2. Kronos æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print(f"  3. è®¾å¤‡ '{args.device}' ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨ 'cpu'")
        print("\nğŸ’¡ å»ºè®®:")
        print("  python stock_predictor.py --device cpu")
        return

    # 2. ç¡®å®šæ•°æ®æ–‡ä»¶è·¯å¾„
    if args.data_path:
        data_path = args.data_path
        symbol = args.symbol
        print(f"\nğŸ“‚ ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®æ–‡ä»¶: {data_path}")
        print(f"ğŸ“ˆ è‚¡ç¥¨ä»£ç : {symbol}")
    else:
        # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
        data_path = os.path.join("examples", "data", "XSHG_5min_600977.csv")
        symbol = "600977"
        print(f"\nğŸ“‚ ä½¿ç”¨ç¤ºä¾‹æ•°æ®æ–‡ä»¶: {data_path}")
        print(f"ğŸ“ˆ è‚¡ç¥¨ä»£ç : {symbol}")

    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        print("\nğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
        if args.data_path:
            print("  1. æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
            print("  2. ç¡®ä¿æ–‡ä»¶åŒ…å«å¿…è¦çš„åˆ—: timestamps, open, high, low, close, volume, amount")
        else:
            print("  1. è¿è¡Œæ•°æ®è·å–è„šæœ¬è·å–è‚¡ç¥¨æ•°æ®:")
            print("     python my_stock_predictor/run_my_prediction.py")
        return

    # 3. åŠ è½½æ•°æ®
    print(f"\nğŸ“– æ­£åœ¨åŠ è½½æ•°æ®...")
    df = predictor.load_data(data_path)
    if df is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return

    # 4. å‡†å¤‡é¢„æµ‹æ•°æ®
    lookback = args.lookback
    pred_len = args.pred_len
    is_future_forecast = args.future_forecast

    print(f"\nâš™ï¸ é¢„æµ‹å‚æ•°:")
    print(f"   - å†å²æ•°æ®ç‚¹: {lookback}")
    print(f"   - é¢„æµ‹é•¿åº¦: {pred_len}")
    print(f"   - é¢„æµ‹æ¨¡å¼: {'æœªæ¥é¢„æµ‹' if is_future_forecast else 'å†å²å›æµ‹'}")

    # æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿ
    if len(df) < lookback + pred_len:
        print(f"âš ï¸ è­¦å‘Š: æ•°æ®ç‚¹ä¸è¶³ (éœ€è¦ {lookback + pred_len}, å®é™… {len(df)})")
        # è‡ªåŠ¨è°ƒæ•´å‚æ•°
        available_points = len(df)
        lookback = min(lookback, available_points // 2)
        pred_len = min(pred_len, available_points - lookback)
        print(f"ğŸ”§ è‡ªåŠ¨è°ƒæ•´å‚æ•°: lookback={lookback}, pred_len={pred_len}")

    x_df, x_timestamp, y_timestamp = predictor.prepare_prediction_data(df, lookback, pred_len)

    # 5. è¿è¡Œé¢„æµ‹æµç¨‹
    print("\nğŸ”® å¼€å§‹é¢„æµ‹æµç¨‹...")
    start_time = datetime.now()

    results = predictor.run_prediction_pipeline(
        historical_df=df,
        x_df=x_df,
        x_timestamp=x_timestamp,
        y_timestamp=y_timestamp,
        is_future_forecast=is_future_forecast,
        symbol=symbol,
        pred_len=pred_len,
        T=1.0,
        top_p=0.9,
        sample_count=1,
        plot_lookback=lookback
    )

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    if results:
        print(f"\nğŸ‰ é¢„æµ‹å®Œæˆï¼ç”¨æ—¶ {duration:.1f} ç§’")
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ° prediction_results/{symbol}/ ç›®å½•")
        print("   - æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨å’Œæ•°æ®æ–‡ä»¶")
        # æ‰“å°ç»“æœæ¦‚è§ˆ
        analysis = results.get('analysis', {})
        if analysis:
            price_change_pct = analysis.get('price_analysis', {}).get('price_change_percentage', 0)
            trend = analysis.get('price_analysis', {}).get('trend', 'æœªçŸ¥')
            print("\nğŸ“Š é¢„æµ‹æ¦‚è§ˆ:")
            print(f"   - ä»·æ ¼å˜åŒ–: {price_change_pct:.2f}%")
            print(f"   - è¶‹åŠ¿: {trend}")
    else:
        print(f"\nâŒ é¢„æµ‹æµç¨‹å¤±è´¥ï¼Œç”¨æ—¶ {duration:.1f} ç§’")

if __name__ == "__main__":
    main()
