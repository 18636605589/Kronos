#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨é¢„æµ‹åˆ†ææ¨¡å—
åŸºäºKronosæ¨¡å‹è¿›è¡Œè‚¡ç¥¨é¢„æµ‹å¹¶ä¿å­˜ç»“æœ
"""

import pandas as pd
import numpy as np
import os
import sys
import json
import argparse
from datetime import datetime, timedelta
import torch
import warnings
import logging
from typing import Optional, Dict, List, Tuple, Any
warnings.filterwarnings('ignore')

# å»¶è¿Ÿå¯¼å…¥matplotlibï¼Œé¿å…åˆå§‹åŒ–å†²çª
# import matplotlib.pyplot as plt  # å°†åœ¨éœ€è¦æ—¶å¯¼å…¥

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

try:
    from model import Kronos, KronosTokenizer, KronosPredictor
except ImportError as e:
    print(f"é”™è¯¯: æ— æ³•å¯¼å…¥Kronosæ¨¡å‹: {e}")
    print("è¯·ç¡®ä¿modelç›®å½•å­˜åœ¨ä¸”åŒ…å«å¿…è¦çš„æ–‡ä»¶")
    raise

# å¯¼å…¥å¸¸é‡
from utils.technical_analysis import TechnicalAnalyzer
from constants import (
    REQUIRED_COLUMNS,
    TIMESTAMP_COLUMN,
    PRICE_COLUMNS,
    DEFAULT_SMOOTH_ALPHA,
    OUTLIER_THRESHOLD,
    MAX_NAN_RATIO,
    MIN_DATA_POINTS,
    DEFAULT_PLOT_LOOKBACK_DAYS,
    FOCUS_MODE_MARGIN_DAYS
)

class StockPredictor:
    """è‚¡ç¥¨é¢„æµ‹å™¨"""
    
    def __init__(self, device="auto", max_context=512, results_dir="prediction_results", enable_adaptive_tuning=True):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨

        Args:
            device (str): è®¡ç®—è®¾å¤‡ï¼Œå¯ä¸º 'cpu'ã€'cuda:0' æˆ– 'auto'
            max_context (int): æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            results_dir (str): ç»“æœä¿å­˜ç›®å½•
            enable_adaptive_tuning (bool): æ˜¯å¦å¯ç”¨è‡ªé€‚åº”å‚æ•°è°ƒä¼˜
        """
        # åˆå§‹åŒ–æ—¥å¿—
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        self.requested_device = device
        self.device = self._resolve_device(device)
        self.max_context = max_context
        self.results_dir = results_dir
        self.enable_adaptive_tuning = enable_adaptive_tuning

        # æ¨¡å‹ç›¸å…³
        self.model = None
        self.tokenizer = None
        self.predictor = None

        # æ€§èƒ½ç›‘æ§
        self.performance_stats = {
            'predictions_count': 0,
            'total_inference_time': 0,
            'memory_peak': 0,
            'errors_count': 0
        }
        
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        self.ensure_results_dir()
        
        # åˆå§‹åŒ–æ¨¡å‹
        try:
            self.load_model()
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–Kronosæ¨¡å‹: {str(e)}")

        # æ•°æ®é¢„å¤„ç†é…ç½®
        self.data_config = {
            'required_columns': ['open', 'high', 'low', 'close', 'volume', 'amount'],
            'timestamp_column': 'timestamps',
            'max_nan_ratio': 0.05,  # æœ€å¤§å…è®¸çš„NaNæ¯”ä¾‹
            'min_data_points': 100,  # æœ€å°‘æ•°æ®ç‚¹æ•°
            'outlier_threshold': 3.0  # å¼‚å¸¸å€¼æ£€æµ‹é˜ˆå€¼(æ ‡å‡†å·®å€æ•°)
        }

        # å½’ä¸€åŒ–å‚æ•°å­˜å‚¨ï¼ˆç”¨äºé€†å˜æ¢ï¼‰
        self.normalization_params = {'method': 'none', 'params': {}}
        
        self.logger.info("StockPredictoråˆå§‹åŒ–å®Œæˆ")

    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        return self.performance_stats.copy()

    def reset_performance_stats(self):
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats = {
            'predictions_count': 0,
            'total_inference_time': 0,
            'memory_peak': 0,
            'errors_count': 0
        }

    def optimize_memory_usage(self):
        """å†…å­˜ä¼˜åŒ–"""
        import gc

        try:
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()

            # å¦‚æœä½¿ç”¨GPUï¼Œæ¸…ç©ºç¼“å­˜
            if self.device.startswith('cuda'):
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        self.logger.info("å·²æ¸…ç©ºGPUç¼“å­˜")
                except ImportError:
                    pass
            elif self.device == 'mps':
                try:
                    import torch
                    if torch.backends.mps.is_available():
                        torch.mps.empty_cache()
                        self.logger.info("å·²æ¸…ç©ºMPSç¼“å­˜")
                except ImportError:
                    pass

            # é¢å¤–çš„å†…å­˜ä¼˜åŒ–æªæ–½
            import psutil
            import os

            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            memory_mb = memory_info.rss / 1024 / 1024

            self.logger.info(f"å½“å‰å†…å­˜ä½¿ç”¨: {memory_mb:.1f} MB")

            # å¦‚æœå†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå°è¯•é‡Šæ”¾æ›´å¤šèµ„æº
            if memory_mb > 8000:  # è¶…è¿‡8GB
                self.logger.warning("å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå°è¯•æ·±åº¦æ¸…ç†...")

                # æ¸…ç†å¯èƒ½å­˜åœ¨çš„ä¸´æ—¶å˜é‡
                if hasattr(self, 'temp_data'):
                    delattr(self, 'temp_data')

                # å†æ¬¡åƒåœ¾å›æ”¶
                gc.collect()

                # åœ¨MPSä¸Šå°è¯•æ›´æ¿€è¿›çš„æ¸…ç†
                if self.device == 'mps':
                    try:
                        import torch
                        # å¼ºåˆ¶åŒæ­¥
                        torch.mps.synchronize()
                        torch.mps.empty_cache()
                        self.logger.info("å·²æ‰§è¡ŒMPSæ·±åº¦æ¸…ç†")
                    except:
                        pass

        except Exception as e:
            self.logger.warning(f"å†…å­˜ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºç°è­¦å‘Š: {str(e)}")
    
    def ensure_results_dir(self):
        """ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.results_dir):
            os.makedirs(self.results_dir)
            print(f"åˆ›å»ºç»“æœç›®å½•: {self.results_dir}")
    
    def _resolve_device(self, device):
        """æ ¹æ®å½“å‰ç¯å¢ƒè§£æå®é™…ä½¿ç”¨çš„è®¾å¤‡"""
        normalized = (device or "auto").lower()

        print(f"ğŸ” è®¾å¤‡æ£€æµ‹: è¯·æ±‚è®¾å¤‡='{normalized}'")

        if normalized == "auto":
            if torch.backends.mps.is_available() and torch.backends.mps.is_built():
                print("âœ… æ£€æµ‹åˆ° Apple Silicon MPSï¼Œè‡ªåŠ¨ä½¿ç”¨ 'mps' è®¾å¤‡ã€‚")
                print("âš ï¸  æ³¨æ„: MPSå¯èƒ½é‡åˆ°å†…å­˜é™åˆ¶ï¼Œå¦‚å¤±è´¥ä¼šè‡ªåŠ¨åˆ‡æ¢åˆ°CPU")
                print("ğŸ’¡ å¦‚éœ€ç›´æ¥ä½¿ç”¨CPUï¼Œè¯·è®¾ç½®: os.environ['DEVICE'] = 'cpu'")
                return "mps"
            elif torch.cuda.is_available():
                print("âœ… æ£€æµ‹åˆ°å¯ç”¨çš„ CUDAï¼Œè‡ªåŠ¨ä½¿ç”¨ 'cuda:0' è®¾å¤‡ã€‚")
                return "cuda:0"
            else:
                print("â„¹ï¸ æœªæ£€æµ‹åˆ° GPU åŠ é€Ÿï¼Œå°†ä½¿ç”¨ CPUã€‚")
                return "cpu"

        if normalized.startswith("cuda"):
            if torch.cuda.is_available():
                return device
            print("âš ï¸ è¯·æ±‚ä½¿ç”¨ CUDAï¼Œä½†å½“å‰ç¯å¢ƒä¸æ”¯æŒï¼Œå·²è‡ªåŠ¨å›é€€åˆ° CPUã€‚")
            return "cpu"

        if normalized == "mps":
            if torch.backends.mps.is_available() and torch.backends.mps.is_built():
                print("âœ… ä½¿ç”¨ MPS (Apple Silicon) è®¾å¤‡ã€‚")
                print("ğŸ’¡ å¦‚æœé‡åˆ°å†…å­˜é—®é¢˜ï¼Œå¯ä»¥åˆ‡æ¢åˆ°CPUæ¨¡å¼ã€‚")
                return "mps"
            print("âš ï¸ è¯·æ±‚ä½¿ç”¨ MPS (Apple Silicon)ï¼Œä½†å½“å‰ç¯å¢ƒä¸æ”¯æŒï¼Œå·²è‡ªåŠ¨å›é€€åˆ° CPUã€‚")
            return "cpu"

        print(f"â„¹ï¸ ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {device}")
        return device
    
    def validate_data(self, df: pd.DataFrame, context: str = "general") -> Tuple[bool, str]:
        """
        éªŒè¯æ•°æ®è´¨é‡å’Œå®Œæ•´æ€§

        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            context: éªŒè¯ä¸Šä¸‹æ–‡æè¿°

        Returns:
            (is_valid, error_message)
        """
        try:
            # 1. æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
            missing_cols = set(self.data_config['required_columns']) - set(df.columns)
            if missing_cols:
                return False, f"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}"

            # 2. æ£€æŸ¥æ—¶é—´æˆ³åˆ—
            if self.data_config['timestamp_column'] not in df.columns:
                return False, f"ç¼ºå°‘æ—¶é—´æˆ³åˆ—: {self.data_config['timestamp_column']}"

            # 3. æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
            if len(df) < self.data_config['min_data_points']:
                return False, f"æ•°æ®ç‚¹è¿‡å°‘: {len(df)} < {self.data_config['min_data_points']}"

            # 4. æ£€æŸ¥NaNå€¼æ¯”ä¾‹
            nan_ratio = df[self.data_config['required_columns']].isnull().sum().sum() / (len(df) * len(self.data_config['required_columns']))
            if nan_ratio > self.data_config['max_nan_ratio']:
                return False, f"NaNå€¼æ¯”ä¾‹è¿‡é«˜: {nan_ratio:.2%} > {self.data_config['max_nan_ratio']:.2%}"

            # 5. æ£€æŸ¥ä»·æ ¼æ•°æ®åˆç†æ€§
            price_cols = ['open', 'high', 'low', 'close']
            for col in price_cols:
                if (df[col] <= 0).any():
                    return False, f"å‘ç°éæ­£ä»·æ ¼å€¼åœ¨åˆ— {col}"

                # æ£€æŸ¥high >= max(open, close), low <= min(open, close)
                if col == 'high':
                    invalid_high = df['high'] < df[['open', 'close']].max(axis=1)
                    if invalid_high.any():
                        return False, f"å‘ç°highä»·æ ¼ä½äºopenæˆ–close"
                elif col == 'low':
                    invalid_low = df['low'] > df[['open', 'close']].min(axis=1)
                    if invalid_low.any():
                        return False, f"å‘ç°lowä»·æ ¼é«˜äºopenæˆ–close"

            # 6. æ£€æŸ¥æ—¶é—´æˆ³æ’åº
            if not df[self.data_config['timestamp_column']].is_monotonic_increasing:
                return False, "æ—¶é—´æˆ³ä¸æ˜¯å•è°ƒé€’å¢çš„"

            return True, "æ•°æ®éªŒè¯é€šè¿‡"

        except Exception as e:
            return False, f"æ•°æ®éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"

    def preprocess_data(self, df: pd.DataFrame, detect_outliers: bool = True,
                       enable_advanced: bool = False, normalization: str = "none",
                       trend_adjustment: bool = False, volatility_filter: bool = False) -> pd.DataFrame:
        """
        æ•°æ®é¢„å¤„ç†å’Œæ¸…ç†ï¼ˆå¢å¼ºç‰ˆï¼‰

        Args:
            df: åŸå§‹æ•°æ®æ¡†
            detect_outliers: æ˜¯å¦æ£€æµ‹å’Œå¤„ç†å¼‚å¸¸å€¼
            enable_advanced: æ˜¯å¦å¯ç”¨é«˜çº§é¢„å¤„ç†
            normalization: å½’ä¸€åŒ–æ–¹æ³• ('standard', 'robust', 'none')
            trend_adjustment: æ˜¯å¦å¯ç”¨è¶‹åŠ¿è°ƒæ•´
            volatility_filter: æ˜¯å¦å¯ç”¨æ³¢åŠ¨ç‡è¿‡æ»¤

        Returns:
            å¤„ç†åçš„æ•°æ®æ¡†
        """
        try:
            logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")

            # é¦–å…ˆéªŒè¯è¾“å…¥æ•°æ®
            is_valid, error_msg = self.validate_data(df, "preprocessing_input")
            if not is_valid:
                raise ValueError(f"è¾“å…¥æ•°æ®éªŒè¯å¤±è´¥: {error_msg}")

            processed_df = df.copy()

            # 1. å¤„ç†ç¼ºå¤±å€¼
            numeric_cols = REQUIRED_COLUMNS
            for col in numeric_cols:
                if processed_df[col].isnull().any():
                    # ä½¿ç”¨å‰å‘å¡«å……ï¼Œç„¶ååå‘å¡«å……
                    processed_df[col] = processed_df[col].ffill().bfill()
                    # å¦‚æœä»æœ‰NaNï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……
                    if processed_df[col].isnull().any():
                        median_val = processed_df[col].median()
                        processed_df[col] = processed_df[col].fillna(median_val)
                        logger.warning(f"åˆ— {col} ä½¿ç”¨ä¸­ä½æ•° {median_val:.2f} å¡«å……å‰©ä½™NaNå€¼")

            # 2. å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†
            if detect_outliers:
                processed_df = self._handle_outliers(processed_df)

            # 3. é«˜çº§é¢„å¤„ç†ï¼ˆå¯é€‰ï¼‰
            if enable_advanced:
                processed_df = self._advanced_preprocessing(
                    processed_df, normalization, trend_adjustment, volatility_filter
                )

            # 4. æ·»åŠ æ•°æ®å¹³æ»‘å¤„ç†ï¼ˆä¼˜åŒ–æªæ–½ï¼‰
            processed_df = self._smooth_price_data(processed_df)

            # 5. ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            processed_df[TIMESTAMP_COLUMN] = pd.to_datetime(processed_df[TIMESTAMP_COLUMN])
            for col in numeric_cols:
                processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')

            # 6. æ’åºæ•°æ®
            processed_df = processed_df.sort_values(TIMESTAMP_COLUMN).reset_index(drop=True)

            # 7. æœ€ç»ˆéªŒè¯å¤„ç†åçš„æ•°æ®
            is_valid, error_msg = self.validate_data(processed_df, "preprocessing_output")
            if not is_valid:
                logger.warning(f"é¢„å¤„ç†åæ•°æ®å­˜åœ¨é—®é¢˜: {error_msg}ï¼Œä½†ç»§ç»­æ‰§è¡Œ")

            logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œå¤„ç†åæ•°æ®é‡: {len(processed_df)}")
            return processed_df

        except Exception as e:
            logger.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}")
            raise

    def _advanced_preprocessing(self, df: pd.DataFrame, normalization: str = "none",
                               trend_adjustment: bool = False, volatility_filter: bool = False) -> pd.DataFrame:
        """
        é«˜çº§æ•°æ®é¢„å¤„ç†æ–¹æ³•

        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            normalization: å½’ä¸€åŒ–æ–¹æ³•
            trend_adjustment: æ˜¯å¦è¶‹åŠ¿è°ƒæ•´
            volatility_filter: æ˜¯å¦æ³¢åŠ¨ç‡è¿‡æ»¤

        Returns:
            å¤„ç†åçš„æ•°æ®æ¡†
        """
        processed_df = df.copy()

        # 1. ä»·æ ¼å½’ä¸€åŒ–
        if normalization != "none":
            processed_df = self._normalize_prices(processed_df, method=normalization)

        # 2. è¶‹åŠ¿è°ƒæ•´
        if trend_adjustment:
            processed_df = self._adjust_trend(processed_df)

        # 3. æ³¢åŠ¨ç‡è¿‡æ»¤
        if volatility_filter:
            processed_df = self._filter_volatility(processed_df)

        return processed_df

    def _normalize_prices(self, df: pd.DataFrame, method: str = "robust") -> pd.DataFrame:
        """ä»·æ ¼å½’ä¸€åŒ– - ä¿å­˜å‚æ•°ç”¨äºåç»­é€†å˜æ¢"""
        normalized_df = df.copy()
        price_cols = PRICE_COLUMNS
        
        # åˆå§‹åŒ–å½’ä¸€åŒ–å‚æ•°å­˜å‚¨
        self.normalization_params = {'method': method, 'params': {}}

        for col in price_cols:
            if method == "standard":
                # Z-scoreæ ‡å‡†åŒ–
                mean_val = df[col].mean()
                std_val = df[col].std()
                self.normalization_params['params'][col] = {
                    'mean': float(mean_val), 
                    'std': float(std_val)
                }
                if std_val > 0:
                    normalized_df[col] = (df[col] - mean_val) / std_val
            elif method == "robust":
                # ç¨³å¥æ ‡å‡†åŒ–ï¼ˆä½¿ç”¨ä¸­ä½æ•°å’ŒIQRï¼‰
                median_val = df[col].median()
                q75, q25 = df[col].quantile([0.75, 0.25])
                iqr = q75 - q25
                self.normalization_params['params'][col] = {
                    'median': float(median_val),
                    'iqr': float(iqr)
                }
                if iqr > 0:
                    normalized_df[col] = (df[col] - median_val) / iqr

        logger.info(f"å·²åº”ç”¨{method}ä»·æ ¼å½’ä¸€åŒ–å¹¶ä¿å­˜å‚æ•°")
        logger.debug(f"å½’ä¸€åŒ–å‚æ•°: {self.normalization_params}")
        return normalized_df

    def _adjust_trend(self, df: pd.DataFrame) -> pd.DataFrame:
        """è¶‹åŠ¿è°ƒæ•´ - å»é™¤é•¿æœŸè¶‹åŠ¿ï¼Œçªå‡ºå‘¨æœŸæ€§å˜åŒ–"""
        adjusted_df = df.copy()

        # è®¡ç®—ç§»åŠ¨å¹³å‡è¶‹åŠ¿
        for col in PRICE_COLUMNS:
            trend = df[col].rolling(window=50, center=True).mean()
            # å»é™¤è¶‹åŠ¿æˆåˆ†
            adjusted_df[col] = df[col] - trend + trend.mean()

        logger.info("å·²åº”ç”¨è¶‹åŠ¿è°ƒæ•´")
        return adjusted_df.fillna(method='bfill').fillna(method='ffill')

    def _filter_volatility(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ³¢åŠ¨ç‡è¿‡æ»¤ - å‡å°‘é«˜æ³¢åŠ¨æœŸçš„å½±å“"""
        filtered_df = df.copy()

        # è®¡ç®—æ»šåŠ¨æ³¢åŠ¨ç‡
        returns = df['close'].pct_change()
        volatility = returns.rolling(window=20).std()

        # é«˜æ³¢åŠ¨æœŸæƒé‡é™ä½
        volatility_threshold = volatility.quantile(0.8)  # 80åˆ†ä½æ•°
        weights = 1 / (1 + volatility / volatility_threshold)

        # åº”ç”¨æƒé‡åˆ°ä»·æ ¼æ•°æ®
        for col in PRICE_COLUMNS:
            filtered_df[col] = df[col] * weights + df[col] * (1 - weights)

        logger.info("å·²åº”ç”¨æ³¢åŠ¨ç‡è¿‡æ»¤")
        return filtered_df.fillna(method='bfill').fillna(method='ffill')
    
    def _inverse_normalization(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        é€†å½’ä¸€åŒ– - å°†å½’ä¸€åŒ–åçš„æ•°æ®è¿˜åŸåˆ°åŸå§‹ä»·æ ¼å°ºåº¦
        
        Args:
            df: å½’ä¸€åŒ–åçš„æ•°æ®æ¡†
            
        Returns:
            è¿˜åŸåˆ°åŸå§‹å°ºåº¦çš„æ•°æ®æ¡†
        """
        # å¦‚æœæ²¡æœ‰è¿›è¡Œå½’ä¸€åŒ–ï¼Œç›´æ¥è¿”å›
        if not self.normalization_params or self.normalization_params.get('method') == 'none':
            logger.debug("æœªè¿›è¡Œå½’ä¸€åŒ–ï¼Œè·³è¿‡é€†å˜æ¢")
            return df
        
        denormalized_df = df.copy()
        method = self.normalization_params['method']
        params = self.normalization_params.get('params', {})
        
        if not params:
            logger.warning("å½’ä¸€åŒ–å‚æ•°ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œé€†å˜æ¢")
            return df
        
        # å¯¹æ¯ä¸ªä»·æ ¼åˆ—è¿›è¡Œé€†å˜æ¢
        for col in PRICE_COLUMNS:
            if col in df.columns and col in params:
                col_params = params[col]
                if method == "standard":
                    # Z-scoreé€†å˜æ¢: x = z * std + mean
                    denormalized_df[col] = df[col] * col_params['std'] + col_params['mean']
                    logger.debug(f"åˆ— {col} é€†æ ‡å‡†åŒ–: mean={col_params['mean']:.4f}, std={col_params['std']:.4f}")
                elif method == "robust":
                    # Robusté€†å˜æ¢: x = z * IQR + median
                    denormalized_df[col] = df[col] * col_params['iqr'] + col_params['median']
                    logger.debug(f"åˆ— {col} é€†ç¨³å¥æ ‡å‡†åŒ–: median={col_params['median']:.4f}, iqr={col_params['iqr']:.4f}")
        
        logger.info(f"å·²åº”ç”¨{method}é€†å½’ä¸€åŒ–ï¼Œæ•°æ®è¿˜åŸåˆ°åŸå§‹ä»·æ ¼å°ºåº¦")
        return denormalized_df

    def _smooth_price_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å¯¹ä»·æ ¼æ•°æ®è¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œå‡å°‘å™ªå£°ï¼Œæé«˜é¢„æµ‹ç¨³å®šæ€§
        """
        smoothed_df = df.copy()
        price_cols = ['open', 'high', 'low', 'close']

        for col in price_cols:
            if col in smoothed_df.columns:
                # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡è¿›è¡Œè½»å¾®å¹³æ»‘ï¼ˆalpha=0.1ï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰
                smoothed_df[col] = smoothed_df[col].ewm(alpha=0.1, adjust=True).mean()

        logger.info("å·²åº”ç”¨æ•°æ®å¹³æ»‘å¤„ç†ä»¥æé«˜é¢„æµ‹ç¨³å®šæ€§")
        return smoothed_df

    def _handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å¤„ç†å¼‚å¸¸å€¼ - ä½¿ç”¨IQRæ–¹æ³•è¿›è¡Œæ›´robustçš„æ£€æµ‹
        """
        processed_df = df.copy()
        price_cols = ['open', 'high', 'low', 'close']

        for col in price_cols:
            # ä½¿ç”¨IQR (å››åˆ†ä½è·) æ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼ï¼Œæ›´é€‚åˆè‚¡ç¥¨æ•°æ®
            Q1 = processed_df[col].quantile(0.25)
            Q3 = processed_df[col].quantile(0.75)
            IQR = Q3 - Q1

            # IQRå¼‚å¸¸å€¼æ£€æµ‹ï¼šä½äºQ1-1.5*IQRæˆ–é«˜äºQ3+1.5*IQRçš„å€¼
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outlier_mask = (processed_df[col] < lower_bound) | (processed_df[col] > upper_bound)

            if outlier_mask.any():
                outlier_count = outlier_mask.sum()
                logger.warning(f"åˆ— {col} ä½¿ç”¨IQRæ–¹æ³•å‘ç° {outlier_count} ä¸ªå¼‚å¸¸å€¼")

                # å¯¹äºä»·æ ¼æ•°æ®ï¼Œä½¿ç”¨å±€éƒ¨å›å½’æˆ–æ’å€¼è€Œä¸æ˜¯ç®€å•çš„ç§»åŠ¨å¹³å‡
                if outlier_count / len(processed_df) < 0.05:  # å¼‚å¸¸å€¼æ¯”ä¾‹å°äº5%
                    # ä½¿ç”¨çº¿æ€§æ’å€¼æ›¿æ¢å¼‚å¸¸å€¼
                    processed_df[col] = processed_df[col].where(~outlier_mask, np.nan)
                    processed_df[col] = processed_df[col].interpolate(method='linear', limit_direction='both')
                else:
                    # å¦‚æœå¼‚å¸¸å€¼å¤ªå¤šï¼Œä½¿ç”¨ç§»åŠ¨ä¸­ä½æ•°ï¼ˆæ›´robustï¼‰
                    processed_df[col] = processed_df[col].where(~outlier_mask, processed_df[col].rolling(window=7, center=True, min_periods=3).median())

                # ç¡®ä¿æ’å€¼åæ²¡æœ‰NaNå€¼
                if processed_df[col].isnull().any():
                    processed_df[col] = processed_df[col].ffill().bfill()

        return processed_df
    
    def load_model(self):
        """åŠ è½½Kronosæ¨¡å‹ï¼Œæ”¯æŒç¦»çº¿æ¨¡å¼å’Œè‡ªåŠ¨æ›´æ–°"""
        try:
            print(f"æ­£åœ¨åŠ è½½Kronosæ¨¡å‹... (device: {self.device})")

            # ç¦»çº¿æ¨¡å¼åŠ è½½é€»è¾‘
            offline_mode = os.environ.get('KRONOS_OFFLINE_MODE', 'false').lower() == 'true'

            if offline_mode:
                print("ğŸ”Œ å¯ç”¨ç¦»çº¿æ¨¡å¼ï¼Œåªä½¿ç”¨æœ¬åœ°ç¼“å­˜çš„æ¨¡å‹")
                self._load_model_offline()
            else:
                print("ğŸŒ å¯ç”¨åœ¨çº¿æ¨¡å¼ï¼Œä¼˜å…ˆä½¿ç”¨æœ€æ–°æ¨¡å‹")
                self._load_model_with_update()

            # åˆ›å»ºé¢„æµ‹å™¨
            self.predictor = KronosPredictor(
                self.model,
                self.tokenizer,
                device=self.device,
                max_context=self.max_context
            )

            print("âœ… Kronoså¤§æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            print(f"   ğŸ“Š æ¨¡å‹ä¿¡æ¯: {self.model.__class__.__name__}")
            print(f"   ğŸ–¥ï¸  è¿è¡Œè®¾å¤‡: {self.device}")
            print(f"   ğŸ§  æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦: {self.max_context}")

        except ImportError as e:
            print(f"âŒ æ¨¡å‹å¯¼å…¥å¤±è´¥: {e}")
            print("\nğŸ”§ è§£å†³æ–¹æ¡ˆ:")
            print("  1. ç¡®ä¿å·²å®‰è£…Kronosç›¸å…³ä¾èµ–: pip install -r requirements.txt")
            print("  2. æ£€æŸ¥modelç›®å½•æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«å¿…è¦çš„æ–‡ä»¶")
            raise RuntimeError(f"Kronosæ¨¡å‹å¯¼å…¥å¤±è´¥: {e}")

        except ConnectionError as e:
            print(f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥: {e}")
            print("\nğŸ”§ è§£å†³æ–¹æ¡ˆ:")
            print("  1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            print("  2. è®¾ç½®ä»£ç†: export HTTPS_PROXY=http://your-proxy:port")
            print("  3. æˆ–ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°åä½¿ç”¨ç¦»çº¿æ¨¡å¼")
            raise RuntimeError(f"æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥: {e}")

        except Exception as e:
            error_type = type(e).__name__
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ ({error_type}): {e}")
            print("\nğŸ”§ è§£å†³æ–¹æ¡ˆ:")

            # æä¾›å…·ä½“çš„è§£å†³æ–¹æ¡ˆ
            if "SSL" in str(e).upper() or "CERTIFICATE" in str(e).upper():
                print("  1. SSLè¯ä¹¦é—®é¢˜ï¼Œå°è¯•ä»¥ä¸‹æ­¥éª¤:")
                print("    - å‡çº§ç½‘ç»œåº“: pip install --upgrade requests urllib3 certifi")
                print("    - è®¾ç½®ä»£ç†: export HTTPS_PROXY=http://your-proxy:port")
                print("    - æˆ–ä¸´æ—¶è·³è¿‡SSLéªŒè¯: export HF_HUB_DISABLE_SSL_VERIFICATION=1")
                print("  2. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®")
            elif "timeout" in str(e).lower() or "connection" in str(e).lower():
                print("  1. ç½‘ç»œè¿æ¥é—®é¢˜:")
                print("    - æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
                print("    - è®¾ç½®ä»£ç†æœåŠ¡å™¨")
                print("    - å°è¯•ä½¿ç”¨VPN")
                print("  2. ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°åç¦»çº¿ä½¿ç”¨")
            elif "disk" in str(e).lower() or "space" in str(e).lower():
                print("  1. ç£ç›˜ç©ºé—´ä¸è¶³:")
                print("    - æ¸…ç†ç£ç›˜ç©ºé—´")
                print("    - è®¾ç½®HF_HOMEåˆ°å…¶ä»–ç›®å½•: export HF_HOME=/path/to/large/disk")
            elif "memory" in str(e).lower() or "cuda" in str(e).lower():
                print("  1. å†…å­˜ä¸è¶³:")
                print("    - ä½¿ç”¨CPUæ¨¡å¼: export DEVICE=cpu")
                print("    - å‡å°‘max_contextå‚æ•°")
                print("    - å…³é—­å…¶ä»–ç¨‹åºé‡Šæ”¾å†…å­˜")
            else:
                print("  1. é€šç”¨è§£å†³æ–¹æ¡ˆ:")
                print("    - æ£€æŸ¥HuggingFace tokenæ˜¯å¦æ­£ç¡®è®¾ç½®")
                print("    - å°è¯•é‡æ–°å®‰è£…ç›¸å…³åº“: pip install --upgrade transformers huggingface-hub")
                print("    - æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†…å­˜å’Œç£ç›˜ç©ºé—´")
                print("    - å°è¯•é‡å¯Pythonç¯å¢ƒ")

            # å¦‚æœæ˜¯SSLé”™è¯¯ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ
            if "SSL" in str(e).upper() and "HF_HUB_DISABLE_SSL_VERIFICATION" not in os.environ:
                print("\nğŸ”„ å°è¯•è‡ªåŠ¨ä¿®å¤SSLé—®é¢˜...")
                try:
                    # å†æ¬¡è®¾ç½®ç¯å¢ƒå˜é‡å¹¶é‡è¯•
                    os.environ['HF_HUB_DISABLE_SSL_VERIFICATION'] = '1'
                    os.environ['REQUESTS_CA_BUNDLE'] = ''
                    os.environ['SSL_CERT_FILE'] = ''
                    print("âœ… å·²è®¾ç½®è·³è¿‡SSLéªŒè¯çš„ç¯å¢ƒå˜é‡ï¼Œè¯·é‡æ–°è¿è¡Œç¨‹åº")
                except Exception as retry_e:
                    print(f"âŒ è‡ªåŠ¨ä¿®å¤å¤±è´¥: {retry_e}")

            raise RuntimeError(f"Kronosæ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    def _load_model_offline(self):
        """ç¦»çº¿æ¨¡å¼ï¼šåªä½¿ç”¨æœ¬åœ°ç¼“å­˜çš„æ¨¡å‹"""
        try:
            print("  ğŸ“‚ å°è¯•åŠ è½½æœ¬åœ°ç¼“å­˜çš„Tokenizer...")
            self.tokenizer = KronosTokenizer.from_pretrained(
                "NeoQuasar/Kronos-Tokenizer-base",
                local_files_only=True
            )
            print("  âœ… TokenizeråŠ è½½æˆåŠŸ")

            print("  ğŸ¤– å°è¯•åŠ è½½æœ¬åœ°ç¼“å­˜çš„æ¨¡å‹...")
            self.model = Kronos.from_pretrained(
                "NeoQuasar/Kronos-base",
                local_files_only=True
            )
            print("  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

        except Exception as e:
            print(f"âŒ ç¦»çº¿æ¨¡å¼åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”§ è§£å†³æ–¹æ¡ˆ:")
            print("  1. ç¡®ä¿æ¨¡å‹å·²ä¸‹è½½åˆ°æœ¬åœ°ç¼“å­˜ (~/.cache/huggingface/hub/)")
            print("  2. æˆ–è€…å…ˆè¿è¡Œä¸€æ¬¡åœ¨çº¿æ¨¡å¼ä¸‹è½½æ¨¡å‹")
            print("  3. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œç£ç›˜ç©ºé—´")
            raise RuntimeError(f"ç¦»çº¿æ¨¡å¼åŠ è½½å¤±è´¥: {e}")

    def _should_update_model(self):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ›´æ–°æ¨¡å‹"""
        # æ£€æŸ¥å¼ºåˆ¶æ›´æ–°æ ‡å¿—
        force_update = os.environ.get('KRONOS_FORCE_UPDATE', 'false').lower() == 'true'
        if force_update:
            print("  ğŸ”„ æ£€æµ‹åˆ°å¼ºåˆ¶æ›´æ–°æ ‡å¿—ï¼Œå°†æ›´æ–°æ¨¡å‹")
            return True

        # æ£€æŸ¥æ›´æ–°é—´éš”ï¼ˆé»˜è®¤7å¤©ï¼‰
        update_interval_days = int(os.environ.get('KRONOS_UPDATE_INTERVAL_DAYS', '7'))

        try:
            # æ£€æŸ¥ç‰ˆæœ¬è·Ÿè¸ªæ–‡ä»¶
            version_file = os.path.join(os.path.dirname(__file__), '.model_version.json')
            if not os.path.exists(version_file):
                print(f"  ğŸ“ é¦–æ¬¡è¿è¡Œï¼Œå°†ä¸‹è½½æœ€æ–°æ¨¡å‹")
                return True

            import json
            with open(version_file, 'r') as f:
                version_info = json.load(f)

            last_update = datetime.fromisoformat(version_info.get('last_update', '2000-01-01T00:00:00'))
            days_since_update = (datetime.now() - last_update).days

            if days_since_update >= update_interval_days:
                print(f"  â° è·ç¦»ä¸Šæ¬¡æ›´æ–°å·²è¿‡å»{days_since_update}å¤©ï¼Œå°†æ£€æŸ¥æ¨¡å‹æ›´æ–°")
                return True
            else:
                print(f"  âœ… æ¨¡å‹åœ¨{update_interval_days - days_since_update}å¤©å†…å·²æ›´æ–°è¿‡ï¼Œè·³è¿‡ç½‘ç»œæ£€æŸ¥")
                return False

        except Exception as e:
            print(f"  âš ï¸ ç‰ˆæœ¬æ£€æŸ¥å¤±è´¥: {e}ï¼Œå°†å°è¯•æ›´æ–°")
            return True

    def _update_version_info(self):
        """æ›´æ–°ç‰ˆæœ¬ä¿¡æ¯"""
        try:
            version_file = os.path.join(os.path.dirname(__file__), '.model_version.json')
            version_info = {
                'last_update': datetime.now().isoformat(),
                'tokenizer_repo': 'NeoQuasar/Kronos-Tokenizer-base',
                'model_repo': 'NeoQuasar/Kronos-base'
            }
            import json
            with open(version_file, 'w') as f:
                json.dump(version_info, f, indent=2)
            print("  ğŸ“ å·²æ›´æ–°ç‰ˆæœ¬ä¿¡æ¯")
        except Exception as e:
            print(f"  âš ï¸ æ›´æ–°ç‰ˆæœ¬ä¿¡æ¯å¤±è´¥: {e}")

    def _load_model_with_update(self):
        """åœ¨çº¿æ¨¡å¼ï¼šæ™ºèƒ½æ›´æ–°æ¨¡å‹ï¼Œå¤±è´¥æ—¶ä½¿ç”¨æœ¬åœ°ç¼“å­˜"""
        from datetime import datetime

        # é¦–å…ˆå°è¯•åŠ è½½æœ¬åœ°ç¼“å­˜çš„æ¨¡å‹ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
        local_tokenizer = None
        local_model = None
        local_available = False

        try:
            print("  ğŸ“‚ æ£€æŸ¥æœ¬åœ°ç¼“å­˜...")
            local_tokenizer = KronosTokenizer.from_pretrained(
                "NeoQuasar/Kronos-Tokenizer-base",
                local_files_only=True
            )
            local_model = Kronos.from_pretrained(
                "NeoQuasar/Kronos-base",
                local_files_only=True
            )
            local_available = True
            print("  âœ… æœ¬åœ°ç¼“å­˜å¯ç”¨")
        except Exception as e:
            print(f"  âš ï¸ æœ¬åœ°ç¼“å­˜ä¸å¯ç”¨: {e}")
            print("  ğŸ“¥ å°†ä¸‹è½½æœ€æ–°æ¨¡å‹")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        if not self._should_update_model():
            # ä¸éœ€è¦æ›´æ–°ï¼Œç›´æ¥ä½¿ç”¨æœ¬åœ°ç¼“å­˜
            if local_available:
                print("  ğŸ”„ ä½¿ç”¨æœ¬åœ°ç¼“å­˜çš„æ¨¡å‹...")
                self.tokenizer = local_tokenizer
                self.model = local_model
                print("  âœ… ä½¿ç”¨æœ¬åœ°ç¼“å­˜ç‰ˆæœ¬")
                return
            else:
                print("  âš ï¸ æœ¬åœ°ç¼“å­˜ä¸å¯ç”¨ï¼Œå°†å¼ºåˆ¶ä¸‹è½½")
                # ç»§ç»­åˆ°ç½‘ç»œä¸‹è½½æµç¨‹

        # å°è¯•ä»ç½‘ç»œæ›´æ–°æ¨¡å‹
        try:
            print("  ğŸŒ ä»ç½‘ç»œä¸‹è½½/æ›´æ–°æ¨¡å‹...")

            # è®¾ç½®ç½‘ç»œä¸‹è½½çš„ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœæ²¡æœ‰è®¾ç½®çš„è¯ï¼‰
            if 'HF_HUB_DISABLE_SSL_VERIFICATION' not in os.environ:
                os.environ['HF_HUB_DISABLE_SSL_VERIFICATION'] = '1'
                os.environ['REQUESTS_CA_BUNDLE'] = ''
                os.environ['SSL_CERT_FILE'] = ''
                os.environ['CURL_CA_BUNDLE'] = ''

            self.tokenizer = KronosTokenizer.from_pretrained("NeoQuasar/Kronos-Tokenizer-base")
            print("  âœ… TokenizeråŠ è½½æˆåŠŸ")

            self.model = Kronos.from_pretrained("NeoQuasar/Kronos-base")
            print("  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

            # æ›´æ–°ç‰ˆæœ¬ä¿¡æ¯
            self._update_version_info()

        except Exception as network_error:
            print(f"  âŒ ç½‘ç»œåŠ è½½å¤±è´¥: {network_error}")

            if local_available:
                print("  ğŸ”„ å›é€€åˆ°æœ¬åœ°ç¼“å­˜çš„æ¨¡å‹...")
                self.tokenizer = local_tokenizer
                self.model = local_model
                print("  âœ… å·²åˆ‡æ¢åˆ°æœ¬åœ°ç¼“å­˜ç‰ˆæœ¬")
            else:
                print("  ğŸ’¥ ç½‘ç»œåŠ è½½å¤±è´¥ä¸”æ— æœ¬åœ°ç¼“å­˜ï¼ŒåŠ è½½å¤±è´¥")
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: ç½‘ç»œé”™è¯¯ä¸”æ— æœ¬åœ°ç¼“å­˜ - {network_error}")

    def load_data(self, filepath):
        """
        åŠ è½½è‚¡ç¥¨æ•°æ®
        
        Args:
            filepath (str): æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            pd.DataFrame: è‚¡ç¥¨æ•°æ®
        """
        try:
            print(f"æ­£åœ¨åŠ è½½æ•°æ®: {filepath}")
            
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(filepath)
            
            # ç¡®ä¿æ—¶é—´æˆ³åˆ—å­˜åœ¨
            if 'timestamps' not in df.columns:
                print("é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸­ç¼ºå°‘timestampsåˆ—")
                return None
            
            # è½¬æ¢æ—¶é—´æˆ³æ ¼å¼
            df['timestamps'] = pd.to_datetime(df['timestamps'])
            
            # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„åˆ—éƒ½å­˜åœ¨
            required_columns = ['timestamps', 'open', 'high', 'low', 'close', 'volume', 'amount']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                print(f"è­¦å‘Š: ç¼ºå°‘åˆ—: {missing_columns}")
                # å°è¯•ç”¨å…¶ä»–åˆ—ååŒ¹é…
                column_mapping = {
                    'Open': 'open',
                    'High': 'high',
                    'Low': 'low',
                    'Close': 'close',
                    'Volume': 'volume',
                    'Amount': 'amount'
                }
                
                for old_col, new_col in column_mapping.items():
                    if old_col in df.columns and new_col not in df.columns:
                        df[new_col] = df[old_col]
            
            # æ•°æ®ç±»å‹è½¬æ¢
            numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'amount']
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # åˆ é™¤æ— æ•ˆæ•°æ®
            df = df.dropna()
            
            # æŒ‰æ—¶é—´æ’åº
            df = df.sort_values('timestamps').reset_index(drop=True)
            
            print(f"æˆåŠŸåŠ è½½ {len(df)} æ¡æ•°æ®")
            print(f"æ—¶é—´èŒƒå›´: {df['timestamps'].min()} åˆ° {df['timestamps'].max()}")
            
            return df
            
        except Exception as e:
            print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    def prepare_prediction_data(self, df, lookback=1500, pred_len=96):
        """
        å‡†å¤‡é¢„æµ‹æ•°æ®
        
        Args:
            df (pd.DataFrame): è‚¡ç¥¨æ•°æ®
            lookback (int): å›çœ‹çª—å£å¤§å°
            pred_len (int): é¢„æµ‹é•¿åº¦
            
        Returns:
            tuple: (è¾“å…¥æ•°æ®, è¾“å…¥æ—¶é—´æˆ³, è¾“å‡ºæ—¶é—´æˆ³)
        """
        if len(df) < lookback + pred_len:
            print(f"è­¦å‘Š: æ•°æ®é•¿åº¦({len(df)})å°äºlookback({lookback}) + pred_len({pred_len})")
            # è°ƒæ•´å‚æ•°
            lookback = min(lookback, len(df) // 2)
            pred_len = min(pred_len, len(df) - lookback)
            print(f"è°ƒæ•´å‚æ•°: lookback={lookback}, pred_len={pred_len}")
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        x_df = df.loc[:lookback-1, ['open', 'high', 'low', 'close', 'volume', 'amount']]
        x_timestamp = df.loc[:lookback-1, 'timestamps']
        y_timestamp = df.loc[lookback:lookback+pred_len-1, 'timestamps']
        
        return x_df, x_timestamp, y_timestamp
    
    def prepare_backtest_data(self, df, lookback=1500, pred_len=96):
        """
        å‡†å¤‡å›æµ‹æ•°æ® - æ­£ç¡®åˆ‡åˆ†è®­ç»ƒå’Œæµ‹è¯•é›†
        
        Args:
            df: å®Œæ•´å†å²æ•°æ®
            lookback: è®­ç»ƒæ•°æ®é•¿åº¦
            pred_len: é¢„æµ‹é•¿åº¦ï¼ˆå›æµ‹é•¿åº¦ï¼‰
            
        Returns:
            tuple: (è®­ç»ƒæ•°æ®, è®­ç»ƒæ—¶é—´æˆ³, é¢„æµ‹æ—¶é—´æˆ³, çœŸå®æµ‹è¯•æ•°æ®)
        """
        if len(df) < lookback + pred_len:
            raise ValueError(f"æ•°æ®é•¿åº¦({len(df)})ä¸è¶³ä»¥è¿›è¡Œå›æµ‹ï¼Œéœ€è¦è‡³å°‘ {lookback + pred_len} ä¸ªæ•°æ®ç‚¹")
        
        # è®­ç»ƒæ•°æ®ï¼šå‰ lookback è¡Œ
        x_df = df.iloc[:lookback][['open', 'high', 'low', 'close', 'volume', 'amount']].copy()
        x_timestamp = df.iloc[:lookback]['timestamps'].copy()
        
        # é¢„æµ‹æ—¶é—´æˆ³ï¼šæ¥ä¸‹æ¥ pred_len è¡Œçš„æ—¶é—´æˆ³
        y_timestamp = df.iloc[lookback:lookback+pred_len]['timestamps'].copy()
        
        # çœŸå®æ•°æ®ï¼šç”¨äºåç»­éªŒè¯ï¼ˆground truthï¼‰
        ground_truth = df.iloc[lookback:lookback+pred_len][['open', 'high', 'low', 'close', 'volume', 'amount']].copy()
        ground_truth.index = y_timestamp.values  # è®¾ç½®ç´¢å¼•ä¸ºæ—¶é—´æˆ³
        
        logger.info(f"å›æµ‹æ•°æ®å‡†å¤‡å®Œæˆ: è®­ç»ƒé›† {len(x_df)} è¡Œ, é¢„æµ‹é›† {len(ground_truth)} è¡Œ")
        logger.info(f"è®­ç»ƒæ—¶é—´èŒƒå›´: {x_timestamp.iloc[0]} è‡³ {x_timestamp.iloc[-1]}")
        logger.info(f"é¢„æµ‹æ—¶é—´èŒƒå›´: {y_timestamp.iloc[0]} è‡³ {y_timestamp.iloc[-1]}")
        
        return x_df, x_timestamp, y_timestamp, ground_truth
    
    def predict(self, x_df, x_timestamp, y_timestamp, pred_len, T=0.3, top_p=0.8, sample_count=5):
        """
        è¿›è¡Œé¢„æµ‹
        
        Args:
            x_df (pd.DataFrame): è¾“å…¥ç‰¹å¾
            x_timestamp (pd.Series): è¾“å…¥æ—¶é—´æˆ³
            y_timestamp (pd.Series): é¢„æµ‹æ—¶é—´æˆ³
            pred_len (int): é¢„æµ‹é•¿åº¦
            T (float): é‡‡æ ·æ¸©åº¦ ä¸¾ä¾‹ï¼šè‹¥é¢„æµ‹ â€œæ˜å¤©æ˜¯å¦ä¸‹é›¨â€ï¼ŒT=0.1 æ—¶ï¼Œæ¨¡å‹ä¼šåšå®šé€‰æ‹©è®­ç»ƒæ•°æ®ä¸­æœ€å¯èƒ½çš„ç»“æœï¼ˆå¦‚ â€œä¸‹é›¨â€ æ¦‚ç‡ 80% åˆ™ç›´æ¥è¾“å‡ºï¼‰ï¼›è‹¥ T è¿‡é«˜ï¼ˆå¦‚ 0.8ï¼‰ï¼Œå¯èƒ½å› éšæœºé‡‡æ ·è¾“å‡ºä½æ¦‚ç‡é€‰é¡¹ï¼ˆå¦‚ â€œæ™´å¤©â€ï¼‰ï¼Œåç¦»çœŸå®è¶‹åŠ¿ã€‚
            top_p (float): æ ¸é‡‡æ ·æ¦‚ç‡  è‹¥åœºæ™¯ä¸­ â€œçœŸç›¸â€ é«˜åº¦å”¯ä¸€ï¼ˆå¦‚é¢„æµ‹å…·ä½“æ•°å€¼ã€æ˜ç¡®åˆ†ç±»ï¼‰ï¼Œtop_p å¯é™è‡³ 0.5-0.6ï¼Œè¿›ä¸€æ­¥èšç„¦ï¼›è‹¥æ•°æ®å­˜åœ¨è½»å¾®ä¸ç¡®å®šæ€§ï¼ˆå¦‚å¤šå› ç´ å½±å“çš„é¢„æµ‹ï¼‰ï¼Œ0.7-0.8 æ›´ç¨³å¦¥ã€‚
            sample_count (int): é‡‡æ ·æ¬¡æ•°
            
        Returns:
            pd.DataFrame: é¢„æµ‹ç»“æœ
        """
        import time
        import os

        start_time = time.time()

        # å°è¯•å¯¼å…¥psutilï¼Œå¦‚æœå¤±è´¥åˆ™é™çº§åˆ°åŸºç¡€ç›‘æ§
        try:
            import psutil
            psutil_available = True
            initial_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB
        except ImportError:
            psutil_available = False
            initial_memory = 0
            self.logger.warning("psutilä¸å¯ç”¨ï¼Œæ€§èƒ½ç›‘æ§åŠŸèƒ½å°†é™çº§")

        try:
            self.logger.info("ğŸ”® å¼€å§‹æ¨¡å‹æ¨ç†...")
            self.performance_stats['predictions_count'] += 1

            # éªŒè¯è¾“å…¥å‚æ•°
            if x_df is None or x_df.empty:
                raise ValueError("è¾“å…¥æ•°æ®ä¸ºç©º")
            if len(y_timestamp) != pred_len:
                raise ValueError(f"é¢„æµ‹æ—¶é—´æˆ³é•¿åº¦({len(y_timestamp)})ä¸é¢„æµ‹é•¿åº¦({pred_len})ä¸åŒ¹é…")

            # è‡ªé€‚åº”å‚æ•°è°ƒæ•´
            T, top_p, sample_count = self._adaptive_parameter_tuning(x_df, T, top_p, sample_count, pred_len, self.enable_adaptive_tuning)

            self.logger.info(f"ä½¿ç”¨å‚æ•°: T={T:.2f}, top_p={top_p:.2f}, sample_count={sample_count}")

            # å†…å­˜ä¼˜åŒ–ï¼šåœ¨å¤§é¢„æµ‹å‰æ¸…ç†å†…å­˜
            if pred_len > 200:
                self.optimize_memory_usage()

            # æ‰§è¡Œé¢„æµ‹ - è°ƒç”¨Kronoså¤§æ¨¡å‹è¿›è¡Œæ¨ç†
            self.logger.info(f"ğŸ§  æ­£åœ¨è°ƒç”¨Kronoså¤§æ¨¡å‹è¿›è¡Œé¢„æµ‹æ¨ç† (é¢„æµ‹é•¿åº¦: {pred_len})...")
            pred_df = self.predictor.predict(
                df=x_df,
                x_timestamp=x_timestamp,
                y_timestamp=y_timestamp,
                pred_len=pred_len,
                T=T,
                top_p=top_p,
                sample_count=sample_count,
                verbose=True  # æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
            )
            self.logger.info("âœ… Kronoså¤§æ¨¡å‹æ¨ç†å®Œæˆï¼")

            if pred_df is not None:
                # åå¤„ç†é¢„æµ‹ç»“æœ
                pred_df = self._postprocess_predictions(pred_df, x_df)
                
                # ã€æ–°å¢ã€‘åº”ç”¨é€†å½’ä¸€åŒ–ï¼Œå°†é¢„æµ‹ç»“æœè¿˜åŸåˆ°åŸå§‹ä»·æ ¼å°ºåº¦
                pred_df = self._inverse_normalization(pred_df)

                # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
                inference_time = time.time() - start_time

                if psutil_available:
                    current_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
                    memory_used = current_memory - initial_memory
                    self.performance_stats['total_inference_time'] += inference_time
                    self.performance_stats['memory_peak'] = max(self.performance_stats['memory_peak'], memory_used)
                    self.logger.info(f"âœ… é¢„æµ‹å®Œæˆå¹¶ä¼˜åŒ–ï¼ (è€—æ—¶: {inference_time:.2f}s, å†…å­˜: {memory_used:.1f}MB)")
                else:
                    self.performance_stats['total_inference_time'] += inference_time
                    self.logger.info(f"âœ… é¢„æµ‹å®Œæˆå¹¶ä¼˜åŒ–ï¼ (è€—æ—¶: {inference_time:.2f}s)")
            else:
                self.performance_stats['errors_count'] += 1
                self.logger.error("âŒ é¢„æµ‹è¿”å›None")

            return pred_df
            
        except (MemoryError, RuntimeError) as e:
            error_msg = str(e).lower()
            if "out of memory" in error_msg or "memory" in error_msg:
                self.logger.error(f"æ£€æµ‹åˆ°å†…å­˜ä¸è¶³é”™è¯¯: {e}")
                self.optimize_memory_usage()

                # å¦‚æœä½¿ç”¨MPSé‡åˆ°å†…å­˜ä¸è¶³ï¼Œå°è¯•åˆ‡æ¢åˆ°CPU
                if self.device == 'mps':
                    self.logger.warning("ğŸ”„ MPSå†…å­˜ä¸è¶³ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼...")
                    try:
                        # é‡æ–°åˆå§‹åŒ–é¢„æµ‹å™¨ä¸ºCPUæ¨¡å¼
                        self.device = 'cpu'
                        self.predictor = KronosPredictor(
                            self.model.to('cpu'),
                            self.tokenizer.to('cpu'),
                            device='cpu',
                            max_context=self.max_context
                        )
                        self.logger.info("âœ… å·²åˆ‡æ¢åˆ°CPUæ¨¡å¼ï¼Œé‡è¯•é¢„æµ‹...")

                        # é€’å½’è°ƒç”¨è‡ªå·±ï¼Œä½¿ç”¨CPUæ¨¡å¼
                        return self.predict(x_df, x_timestamp, y_timestamp, pred_len, T, top_p, sample_count)

                    except Exception as cpu_error:
                        self.logger.error(f"âŒ CPUæ¨¡å¼ä¹Ÿå¤±è´¥: {cpu_error}")
                        return None
                else:
                    self.logger.error("âŒ å½“å‰å·²æ˜¯CPUæ¨¡å¼ï¼Œå†…å­˜ä»ç„¶ä¸è¶³")
                    return None
            else:
                # ä¸æ˜¯å†…å­˜é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
                raise e
        except Exception as e:
            self.performance_stats['errors_count'] += 1
            self.logger.error(f"é¢„æµ‹å¤±è´¥: {str(e)}")
            self.logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", exc_info=True)
            return None
    
    def _adaptive_parameter_tuning(self, x_df, T, top_p, sample_count, pred_len, enable_adaptive=True):
        """
        è‡ªé€‚åº”å‚æ•°è°ƒä¼˜

        æ ¹æ®æ•°æ®ç‰¹å¾å’Œé¢„æµ‹é•¿åº¦è‡ªåŠ¨è°ƒæ•´å‚æ•°
        """
        # å¦‚æœç¦ç”¨è‡ªé€‚åº”è°ƒæ•´ï¼Œç›´æ¥è¿”å›åŸå§‹å‚æ•°
        if not enable_adaptive:
            self.logger.info(f"è‡ªé€‚åº”å‚æ•°è°ƒä¼˜å·²ç¦ç”¨ï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å‚æ•°: T={T}, top_p={top_p}")
            return T, top_p, sample_count

        # è®¡ç®—æ•°æ®çš„æ³¢åŠ¨æ€§
        close_volatility = x_df['close'].pct_change().std()

        # æ ¹æ®æ³¢åŠ¨æ€§è°ƒæ•´å‚æ•°
        if close_volatility > 0.02:  # é«˜æ³¢åŠ¨æ€§
            T = min(T * 0.8, 0.5)  # é™ä½æ¸©åº¦ï¼Œä½¿é¢„æµ‹æ›´ä¿å®ˆ
            top_p = min(top_p * 0.9, 0.6)  # é™ä½å¤šæ ·æ€§
        elif close_volatility < 0.005:  # ä½æ³¢åŠ¨æ€§
            T = max(T * 1.2, 0.3)  # å¯ä»¥é€‚å½“å¢åŠ æ¸©åº¦
            top_p = max(top_p * 1.1, 0.4)

        # æ ¹æ®é¢„æµ‹é•¿åº¦è°ƒæ•´é‡‡æ ·æ¬¡æ•°
        if pred_len > 100:  # é•¿åºåˆ—é¢„æµ‹
            sample_count = min(sample_count + 1, 5)  # å¢åŠ é‡‡æ ·æ¬¡æ•°æé«˜ç¨³å®šæ€§
        elif pred_len < 20:  # çŸ­åºåˆ—é¢„æµ‹
            sample_count = max(sample_count - 1, 1)  # å¯ä»¥å‡å°‘é‡‡æ ·æ¬¡æ•°

        # ç¡®ä¿å‚æ•°åœ¨åˆç†èŒƒå›´å†…
        T = max(0.1, min(T, 2.0))
        top_p = max(0.1, min(top_p, 0.95))
        sample_count = max(1, min(sample_count, 10))

        return T, top_p, sample_count

    def _postprocess_predictions(self, pred_df, x_df):
        """
        é¢„æµ‹ç»“æœåå¤„ç†

        ç¡®ä¿é¢„æµ‹ç»“æœçš„åˆç†æ€§å’Œè¿ç»­æ€§
        """
        processed_df = pred_df.copy()

        # 1. ç¡®ä¿ä»·æ ¼è¿ç»­æ€§ï¼ˆé˜²æ­¢è·³è·ƒï¼‰
        for col in ['open', 'high', 'low', 'close']:
            if col in processed_df.columns:
                # è®¡ç®—ç›¸é‚»é¢„æµ‹å€¼çš„å˜åŒ–ç‡
                pct_change = processed_df[col].pct_change()

                # è¯†åˆ«å¼‚å¸¸å˜åŒ–ï¼ˆè¶…è¿‡50%çš„å•æ­¥å˜åŒ–ï¼‰
                outlier_mask = pct_change.abs() > 0.5

                if outlier_mask.any():
                    logger.warning(f"æ£€æµ‹åˆ° {outlier_mask.sum()} ä¸ªå¼‚å¸¸ä»·æ ¼å˜åŒ–ï¼Œå·²è¿›è¡Œå¹³æ»‘å¤„ç†")

                    # ä½¿ç”¨ç§»åŠ¨å¹³å‡å¹³æ»‘å¼‚å¸¸å€¼
                    processed_df[col] = processed_df[col].where(
                        ~outlier_mask,
                        processed_df[col].rolling(window=3, center=True, min_periods=1).mean()
                    )

        # 2. ç¡®ä¿OHLCå…³ç³»åˆç†
        processed_df['high'] = processed_df[['open', 'close', 'high']].max(axis=1)
        processed_df['low'] = processed_df[['open', 'close', 'low']].min(axis=1)

        # 3. ç¡®ä¿æˆäº¤é‡ä¸ä¸ºè´Ÿæ•°
        if 'volume' in processed_df.columns:
            processed_df['volume'] = processed_df['volume'].clip(lower=0)
        if 'amount' in processed_df.columns:
            processed_df['amount'] = processed_df['amount'].clip(lower=0)

        return processed_df

    def _calculate_smart_xticks(self, timestamps, max_ticks=15, is_future_forecast=False):
        """
        è®¡ç®—æ™ºèƒ½æ—¶é—´è½´åˆ»åº¦ï¼Œè¶Šæ¥è¿‘å½“å‰æ—¶é—´æ˜¾ç¤ºè¶Šè¯¦ç»†

        Args:
            timestamps: æ‰€æœ‰æ—¶é—´æˆ³
            max_ticks: æœ€å¤§åˆ»åº¦æ•°é‡

        Returns:
            æ™ºèƒ½åˆ»åº¦ä½ç½®å’Œæ ‡ç­¾
        """
        if timestamps.empty:
            return [], []

        timestamps = pd.to_datetime(timestamps)
        start_time = timestamps.min()
        end_time = timestamps.max()
        total_duration = end_time - start_time

        # è®¡ç®—å…³é”®æ—¶é—´ç‚¹
        current_time = pd.Timestamp.now()
        pred_start = timestamps[timestamps >= start_time].min() if len(timestamps) > 0 else end_time

        # æ ¹æ®æ—¶é—´è·¨åº¦ç¡®å®šåˆ»åº¦å¯†åº¦
        total_hours = total_duration.total_seconds() / 3600

        if total_hours <= 24:  # 1å¤©å†…
            # æ¯å°æ—¶ä¸€ä¸ªåˆ»åº¦ï¼Œé‡ç‚¹åŒºåŸŸæ¯15åˆ†é’Ÿ
            base_freq = 'H'
            dense_freq = '15min'
        elif total_hours <= 168:  # 1å‘¨å†…
            # æ¯å¤©ä¸€ä¸ªåˆ»åº¦ï¼Œé‡ç‚¹åŒºåŸŸæ¯å°æ—¶
            base_freq = 'D'
            dense_freq = 'H'
        elif total_hours <= 720:  # 1æœˆå†…
            # æ¯å‘¨ä¸€ä¸ªåˆ»åº¦ï¼Œé‡ç‚¹åŒºåŸŸæ¯å¤©
            base_freq = 'W'
            dense_freq = 'D'
        else:  # æ›´é•¿æ—¶é—´
            # æ¯æœˆä¸€ä¸ªåˆ»åº¦ï¼Œé‡ç‚¹åŒºåŸŸæ¯å‘¨
            base_freq = 'M'
            dense_freq = 'W'

        # ç”ŸæˆåŸºç¡€åˆ»åº¦
        base_ticks = pd.date_range(start=start_time, end=end_time, freq=base_freq)

        # åœ¨å…³é”®åŒºåŸŸæ·»åŠ å¯†é›†åˆ»åº¦
        dense_ticks = []

        # é¢„æµ‹å¼€å§‹æ—¶é—´å‰åçš„å¯†é›†åˆ»åº¦
        pred_dense_start = pred_start - pd.Timedelta(hours=min(total_hours * 0.1, 24))
        pred_dense_end = pred_start + pd.Timedelta(hours=min(total_hours * 0.1, 24))
        if pred_dense_start < end_time and pred_dense_end > start_time:
            pred_dense = pd.date_range(
                start=max(pred_dense_start, start_time),
                end=min(pred_dense_end, end_time),
                freq=dense_freq
            )
            dense_ticks.extend(pred_dense)

        # å½“å‰æ—¶é—´å‰åçš„å¯†é›†åˆ»åº¦ï¼ˆå¦‚æœæ˜¯æœªæ¥é¢„æµ‹ï¼‰
        if is_future_forecast and abs(current_time - end_time) < pd.Timedelta(days=30):
            current_dense_start = current_time - pd.Timedelta(hours=min(total_hours * 0.15, 48))
            current_dense_end = min(current_time + pd.Timedelta(hours=min(total_hours * 0.15, 48)), end_time)
            if current_dense_start < end_time and current_dense_end > start_time:
                current_dense = pd.date_range(
                    start=max(current_dense_start, start_time),
                    end=current_dense_end,
                    freq=dense_freq
                )
                dense_ticks.extend(current_dense)

        # åˆå¹¶æ‰€æœ‰åˆ»åº¦å¹¶å»é‡
        all_ticks = sorted(set(base_ticks).union(set(dense_ticks)))
        all_ticks = [t for t in all_ticks if start_time <= t <= end_time]

        # é™åˆ¶åˆ»åº¦æ•°é‡
        if len(all_ticks) > max_ticks:
            # ä¼˜å…ˆä¿ç•™å…³é”®åŒºåŸŸçš„åˆ»åº¦
            key_ticks = []
            other_ticks = []

            for tick in all_ticks:
                if (abs(tick - pred_start) < pd.Timedelta(hours=24) or
                    (is_future_forecast and abs(tick - current_time) < pd.Timedelta(hours=24))):
                    key_ticks.append(tick)
                else:
                    other_ticks.append(tick)

            # ä¿ç•™æ‰€æœ‰å…³é”®åˆ»åº¦ï¼Œç„¶åå‡åŒ€é‡‡æ ·å…¶ä»–åˆ»åº¦
            remaining_slots = max_ticks - len(key_ticks)
            if remaining_slots > 0 and other_ticks:
                step = max(1, len(other_ticks) // remaining_slots)
                sampled_other = other_ticks[::step][:remaining_slots]
                all_ticks = sorted(set(key_ticks + sampled_other))
            else:
                all_ticks = sorted(key_ticks)

        # ç”Ÿæˆåˆ»åº¦æ ‡ç­¾
        tick_labels = []
        for tick in all_ticks:
            if total_hours <= 24:  # 1å¤©å†…æ˜¾ç¤ºæ—¶åˆ†
                tick_labels.append(tick.strftime('%m-%d %H:%M'))
            elif total_hours <= 168:  # 1å‘¨å†…æ˜¾ç¤ºæ—¥æœŸå’Œå°æ—¶
                tick_labels.append(tick.strftime('%m-%d %H:00'))
            else:  # æ›´é•¿æ—¶é—´æ˜¾ç¤ºæ—¥æœŸ
                tick_labels.append(tick.strftime('%m-%d'))

        return all_ticks, tick_labels
    
    def plot_prediction(self, historical_df, pred_df, symbol, is_future_forecast=False, save_plot=True,
                       plot_lookback=1500, enable_focus_mode=False, plot_lookback_days=None, prediction_highlight=True,
                       raw_historical_df=None):
        """
        ç»˜åˆ¶é¢„æµ‹ç»“æœ - æ™ºèƒ½æ—¶é—´è½´æ˜¾ç¤ºï¼ŒåŒºåˆ†é¢„æµ‹å’Œå›æµ‹æ¨¡å¼
        """
        try:
            logger.info("å¼€å§‹ç»˜åˆ¶é¢„æµ‹å›¾è¡¨...")
            logger.info(f"è¾“å…¥å‚æ•°: symbol={symbol}, is_future_forecast={is_future_forecast}, save_plot={save_plot}, plot_lookback={plot_lookback}")
            logger.info(f"å†å²æ•°æ®å½¢çŠ¶: {historical_df.shape if historical_df is not None else 'None'}")
            logger.info(f"é¢„æµ‹æ•°æ®å½¢çŠ¶: {pred_df.shape if pred_df is not None else 'None'}")

            # éªŒè¯è¾“å…¥æ•°æ®
            if pred_df is None or pred_df.empty:
                logger.error("é¢„æµ‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»˜åˆ¶å›¾è¡¨")
                return None

            if historical_df is None or historical_df.empty:
                logger.error("å†å²æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç»˜åˆ¶å›¾è¡¨")
                return None

            # è®¾ç½®matplotlibåç«¯ä¸ºéäº¤äº’å¼
            logger.info("é…ç½®matplotlib...")
            import matplotlib
            matplotlib.use('Agg')  # ç¡®ä¿ä½¿ç”¨éäº¤äº’å¼åç«¯
            import matplotlib.pyplot as plt
            logger.info(f"matplotlibåç«¯: {matplotlib.get_backend()}")

            # è®¾ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False

            logger.info("matplotlibé…ç½®å®Œæˆ")
            
            # å‡†å¤‡ç»˜å›¾æ•°æ®
            start_pred_time = pred_df.index.min()
            logger.info(f"é¢„æµ‹å¼€å§‹æ—¶é—´: {start_pred_time}, ç±»å‹: {type(start_pred_time)}")

            # æ£€æŸ¥æ•°æ®ç±»å‹
            logger.info(f"å†å²æ•°æ®timestampsç±»å‹: {historical_df['timestamps'].dtype}")
            logger.info(f"é¢„æµ‹æ•°æ®ç´¢å¼•ç±»å‹: {pred_df.index.dtype}")

            # ç¡®ä¿æ—¶é—´æˆ³æ ¼å¼ä¸€è‡´å¹¶ç­›é€‰æ•°æ®
            try:
                hist_timestamps = pd.to_datetime(historical_df['timestamps'])
                pred_start = pd.to_datetime(start_pred_time)

                if enable_focus_mode and plot_lookback_days:
                    # ä¸“æ³¨æ¨¡å¼ï¼šåªæ˜¾ç¤ºé¢„æµ‹ç›¸å…³çš„æœ€è¿‘å‡ å¤©æ•°æ®
                    focus_end = pred_df.index.max() + pd.Timedelta(days=FOCUS_MODE_MARGIN_DAYS)
                    focus_start = focus_end - pd.Timedelta(days=plot_lookback_days)
                    historical_plot_df = historical_df[
                        (hist_timestamps >= focus_start) & (hist_timestamps <= focus_end)
                    ]
                    logger.info(f"ä¸“æ³¨æ¨¡å¼: æ˜¾ç¤ºæœ€è¿‘{plot_lookback_days}å¤©çš„å†å²æ•°æ®")
                else:
                    # ä¼ ç»Ÿæ¨¡å¼ï¼šæ˜¾ç¤ºæŒ‡å®šæ•°é‡çš„å†å²æ•°æ®ç‚¹
                    historical_plot_df = historical_df[hist_timestamps < pred_start].tail(plot_lookback)

                logger.info(f"å†å²æ•°æ®ç‚¹æ•°: {len(historical_plot_df)}")
            except Exception as e:
                logger.error(f"æ—¶é—´æˆ³å¤„ç†å¤±è´¥: {e}")
                return None

            # åˆå¹¶æ‰€æœ‰æ—¶é—´ç‚¹ç”¨äºæ™ºèƒ½åˆ»åº¦è®¡ç®—
            timestamps_list = []
            if not historical_plot_df.empty:
                timestamps_list.append(historical_plot_df['timestamps'])
            timestamps_list.append(pd.Series(pred_df.index))

            all_timestamps = pd.concat(timestamps_list).sort_values().drop_duplicates()
            logger.info(f"æ€»æ—¶é—´ç‚¹æ•°: {len(all_timestamps)}")

            # åˆ›å»ºå›¾è¡¨
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8), sharex=True)
            logger.info("matplotlibå›¾å½¢åˆ›å»ºå®Œæˆ")
            
            # --- ä»·æ ¼å›¾ ---
            # ä½¿ç”¨åŸå§‹å†å²æ•°æ®ï¼ˆå¦‚æœæä¾›ï¼‰æ¥ç¡®ä¿æ­£ç¡®çš„ä»·æ ¼å°ºåº¦æ˜¾ç¤º
            plot_hist_df = historical_plot_df
            if raw_historical_df is not None and not raw_historical_df.empty:
                # å°è¯•åŒ¹é…æ—¶é—´æˆ³æ¥è·å–åŸå§‹æ•°æ®
                try:
                    # æ ¹æ®æ—¶é—´æˆ³åŒ¹é…åŸå§‹å†å²æ•°æ®
                    hist_timestamps = historical_plot_df['timestamps']
                    raw_plot_df = raw_historical_df[raw_historical_df['timestamps'].isin(hist_timestamps)]
                    if not raw_plot_df.empty:
                        plot_hist_df = raw_plot_df.copy()
                        logger.info("ä½¿ç”¨åŸå§‹å†å²æ•°æ®è¿›è¡Œå›¾è¡¨ç»˜åˆ¶ï¼Œç¡®ä¿ä»·æ ¼å°ºåº¦æ­£ç¡®")
                except Exception as e:
                    logger.warning(f"æ— æ³•ä½¿ç”¨åŸå§‹æ•°æ®ç»˜å›¾ï¼Œä½¿ç”¨é¢„å¤„ç†æ•°æ®: {e}")

            ax1.plot(plot_hist_df['timestamps'], plot_hist_df['close'], label='å†å²ä»·æ ¼', color='blue', linewidth=1.5)

            # é¢„æµ‹ä»·æ ¼çº¿æ¡ï¼ˆæ”¯æŒé«˜äº®ï¼‰
            if prediction_highlight:
                # é«˜äº®é¢„æµ‹åŒºåŸŸ
                pred_start_time = pred_df.index.min()
                pred_end_time = pred_df.index.max()
                ax1.axvspan(pred_start_time, pred_end_time, alpha=0.1, color='red', label='é¢„æµ‹åŒºåŸŸ')

                # ç»˜åˆ¶é¢„æµ‹çº¿ï¼ˆæ›´ç²—ï¼Œæ›´æ˜æ˜¾çš„æ ·å¼ï¼‰
                ax1.plot(pred_df.index, pred_df['close'], label='é¢„æµ‹ä»·æ ¼',
                        color='red', linewidth=3, linestyle='--', marker='o', markersize=4, alpha=0.9)
            else:
                ax1.plot(pred_df.index, pred_df['close'], label='é¢„æµ‹ä»·æ ¼', color='red', linewidth=2, linestyle='--')
            
            # åœ¨å›æµ‹æ¨¡å¼ä¸‹ï¼Œæ·»åŠ çœŸå®ä»·æ ¼æ›²çº¿
            if not is_future_forecast:
                true_values_df = historical_df[historical_df['timestamps'].isin(pred_df.index)]
                ax1.plot(true_values_df['timestamps'], true_values_df['close'], label='çœŸå®ä»·æ ¼', color='green', linewidth=2, alpha=0.8)

            # æ·»åŠ å…³é”®æ—¶é—´åŒºåŸŸæ ‡è®°
            current_time = pd.Timestamp.now()

            # æ ‡è®°é¢„æµ‹å¼€å§‹æ—¶é—´
            pred_start_time = pred_df.index.min()
            ax1.axvline(pred_start_time, color='orange', linestyle=':', linewidth=1.5, alpha=0.7)
            ax1.text(pred_start_time, ax1.get_ylim()[1]*0.95, 'é¢„æµ‹å¼€å§‹',
                    ha='center', va='top', fontsize=10,
                    bbox=dict(boxstyle='round,pad=0.3', facecolor='orange', alpha=0.7))

            # æ ‡è®°å½“å‰æ—¶é—´ï¼ˆå¦‚æœæ˜¯æœªæ¥é¢„æµ‹ä¸”æ¥è¿‘å½“å‰æ—¶é—´ï¼‰
            if is_future_forecast and abs(current_time - all_timestamps.max()) < pd.Timedelta(days=7):
                ax1.axvline(current_time, color='purple', linestyle=':', linewidth=1.5, alpha=0.7)
                ax1.text(current_time, ax1.get_ylim()[1]*0.90, 'å½“å‰æ—¶é—´',
                        ha='center', va='top', fontsize=10,
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='purple', alpha=0.7))

            # æ ‡è®°å†å²æ•°æ®ç»“æŸæ—¶é—´
            if not historical_plot_df.empty:
                hist_end_time = historical_plot_df['timestamps'].iloc[-1]
                ax1.axvline(hist_end_time, color='gray', linestyle='--', linewidth=1, alpha=0.8)
                ax1.text(hist_end_time, ax1.get_ylim()[0]*1.05, 'å†å²ç»“æŸ',
                        ha='center', va='bottom', fontsize=9,
                        bbox=dict(boxstyle='round,pad=0.2', facecolor='gray', alpha=0.6))
            
            # è‡ªåŠ¨è°ƒæ•´Yè½´èŒƒå›´ï¼Œç¡®ä¿ä»·æ ¼æ˜¾ç¤ºæ¸…æ™°
            all_prices = pd.concat([
                plot_hist_df['close'],
                pred_df['close']
            ])
            if not is_future_forecast:
                true_prices = historical_df[historical_df['timestamps'].isin(pred_df.index)]['close']
                all_prices = pd.concat([all_prices, true_prices])

            price_min, price_max = all_prices.min(), all_prices.max()
            price_range = price_max - price_min
            if price_range > 0:
                # æ·»åŠ 10%çš„è¾¹è·
                margin = price_range * 0.1
                ax1.set_ylim(price_min - margin, price_max + margin)

            ax1.set_ylabel('ä»·æ ¼', fontsize=14)
            mode_name = 'æœªæ¥é¢„æµ‹' if is_future_forecast else 'å†å²å›æµ‹'
            mode_color = 'orange' if is_future_forecast else 'blue'
            mode_desc = 'é¢„æµ‹æœªæ¥è¶‹åŠ¿' if is_future_forecast else 'éªŒè¯å†å²è¡¨ç°'

            ax1.set_title(f'{symbol} è‚¡ç¥¨{mode_name}ç»“æœ - æ™ºèƒ½æ—¶é—´è½´', fontsize=16)

            # åœ¨å›¾è¡¨å·¦ä¸Šè§’æ·»åŠ æ¨¡å¼æ ‡è¯†æ¡†
            ax1.text(0.02, 0.98, f'{mode_name}\n{mode_desc}',
                    transform=ax1.transAxes, fontsize=11, verticalalignment='top',
                    bbox=dict(boxstyle='round,pad=0.5', facecolor=mode_color, alpha=0.8),
                    color='white', fontweight='bold')

            ax1.legend(loc='upper left', fontsize=12)
            ax1.grid(True, alpha=0.3)
            
            # --- æˆäº¤é‡å›¾ ---
            ax2.plot(plot_hist_df['timestamps'], plot_hist_df['volume'], label='å†å²æˆäº¤é‡', color='blue', linewidth=1.5)
            ax2.plot(pred_df.index, pred_df['volume'], label='é¢„æµ‹æˆäº¤é‡', color='red', linewidth=2, linestyle='--')
            
            if not is_future_forecast:
                true_values_df = historical_df[historical_df['timestamps'].isin(pred_df.index)]
                ax2.plot(true_values_df['timestamps'], true_values_df['volume'], label='çœŸå®æˆäº¤é‡', color='green', linewidth=2, alpha=0.8)

            # åœ¨æˆäº¤é‡å›¾ä¸Šä¹Ÿæ·»åŠ å…³é”®æ—¶é—´æ ‡è®°
            ax2.axvline(pred_start_time, color='orange', linestyle=':', linewidth=1.5, alpha=0.7)

            if is_future_forecast and abs(current_time - all_timestamps.max()) < pd.Timedelta(days=7):
                ax2.axvline(current_time, color='purple', linestyle=':', linewidth=1.5, alpha=0.7)

            if not historical_plot_df.empty:
                hist_end_time = historical_plot_df['timestamps'].iloc[-1]
                ax2.axvline(hist_end_time, color='gray', linestyle='--', linewidth=1, alpha=0.8)
            
            ax2.set_ylabel('æˆäº¤é‡', fontsize=14)
            ax2.set_xlabel('æ—¶é—´', fontsize=14)
            ax2.legend(loc='upper left', fontsize=12)
            ax2.grid(True, alpha=0.3)

            # è®¾ç½®æ™ºèƒ½æ—¶é—´è½´åˆ»åº¦
            smart_ticks, tick_labels = self._calculate_smart_xticks(all_timestamps, max_ticks=20, is_future_forecast=is_future_forecast)
            if smart_ticks:
                ax2.set_xticks(smart_ticks)
                ax2.set_xticklabels(tick_labels, rotation=45, ha='right', fontsize=10)

            # æ·»åŠ å±€éƒ¨æ”¾å¤§è§†å›¾ï¼ˆå¦‚æœæ—¶é—´è·¨åº¦è¾ƒå¤§ä¸”é¢„æµ‹åŒºåŸŸè¾ƒçŸ­ï¼‰
            total_duration = all_timestamps.max() - all_timestamps.min()
            pred_duration = pred_df.index.max() - pred_df.index.min()

            if total_duration > pd.Timedelta(days=30) and pred_duration < total_duration * 0.3:
                # åˆ›å»ºå±€éƒ¨æ”¾å¤§å­å›¾
                # è®¡ç®—é¢„æµ‹åŒºåŸŸçš„æ‰©å±•èŒƒå›´ï¼ˆå‰åå„å»¶é•¿10%ï¼‰
                pred_range = pred_df.index.max() - pred_df.index.min()
                zoom_margin = pred_range * 0.2

                zoom_start = max(all_timestamps.min(), pred_df.index.min() - zoom_margin)
                zoom_end = min(all_timestamps.max(), pred_df.index.max() + zoom_margin)

                # åˆ›å»ºå­å›¾ä½ç½®
                from mpl_toolkits.axes_grid1.inset_locator import inset_axes
                ax_zoom = inset_axes(ax1, width="35%", height="25%", loc='upper right',
                                    bbox_to_anchor=(0.02, 0.02, 0.96, 0.96),
                                    bbox_transform=ax1.transAxes)

                # åœ¨å­å›¾ä¸­ç»˜åˆ¶å±€éƒ¨æ”¾å¤§çš„ä»·æ ¼æ•°æ®
                zoom_hist = historical_plot_df[
                    (historical_plot_df['timestamps'] >= zoom_start) &
                    (historical_plot_df['timestamps'] <= zoom_end)
                ]
                zoom_pred = pred_df[
                    (pred_df.index >= zoom_start) &
                    (pred_df.index <= zoom_end)
                ]

                if not zoom_hist.empty:
                    ax_zoom.plot(zoom_hist['timestamps'], zoom_hist['close'],
                               color='blue', linewidth=1, alpha=0.7)
                if not zoom_pred.empty:
                    ax_zoom.plot(zoom_pred.index, zoom_pred['close'],
                               color='red', linewidth=1.5, linestyle='--', alpha=0.8)

                # å­å›¾æ ·å¼è®¾ç½®
                ax_zoom.set_title('é¢„æµ‹åŒºåŸŸæ”¾å¤§', fontsize=9, pad=2)
                ax_zoom.tick_params(labelsize=8)
                ax_zoom.grid(True, alpha=0.3)

                # æ·»åŠ è¾¹æ¡†
                for spine in ax_zoom.spines.values():
                    spine.set_edgecolor('orange')
                    spine.set_linewidth(1)

            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            plot_path = None
            if save_plot:
                try:
                    # --- åˆ›å»ºæ¨¡å¼åŒºåˆ†çš„ç»“æœå­æ–‡ä»¶å¤¹ ---
                    mode_dir = 'future_forecast' if is_future_forecast else 'backtest'
                    symbol_results_dir = os.path.join(self.results_dir, symbol, mode_dir)
                    os.makedirs(symbol_results_dir, exist_ok=True)
                    logger.info(f"åˆ›å»ºç»“æœç›®å½•: {symbol_results_dir}")

                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    mode_prefix = 'forecast' if is_future_forecast else 'backtest'
                    plot_filename = f"{symbol}_{mode_prefix}_chart_{timestamp}.png"
                    plot_path = os.path.join(symbol_results_dir, plot_filename)

                    logger.info(f"æ­£åœ¨ä¿å­˜å›¾è¡¨åˆ°: {plot_path}")
                    plt.savefig(plot_path, dpi=300, bbox_inches='tight')

                    # éªŒè¯æ–‡ä»¶æ˜¯å¦æˆåŠŸä¿å­˜
                    if os.path.exists(plot_path):
                        file_size = os.path.getsize(plot_path)
                        logger.info(f"âœ… å›¾è¡¨ä¿å­˜æˆåŠŸ: {plot_path} ({file_size} bytes)")
                    else:
                        logger.error(f"âŒ å›¾è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {plot_path}")
                        plot_path = None

                except Exception as save_error:
                    logger.error(f"ä¿å­˜å›¾è¡¨å¤±è´¥: {str(save_error)}")
                    plot_path = None

            # åœ¨éäº¤äº’ç¯å¢ƒä¸­ä¸æ˜¾ç¤ºå›¾è¡¨ï¼Œé¿å…é˜»å¡
            # æ³¨æ„ï¼šæˆ‘ä»¬å·²ç»è®¾ç½®äº†Aggåç«¯ï¼Œæ‰€ä»¥plt.show()ä¸ä¼šå®é™…æ˜¾ç¤ºå›¾åƒ
            try:
                plt.show()  # åœ¨Aggåç«¯ä¸‹è¿™ä¸ä¼šæ˜¾ç¤ºä»»ä½•ä¸œè¥¿ï¼Œåªæ˜¯ä¸ºäº†å…¼å®¹æ€§
                logger.info("å›¾è¡¨æ˜¾ç¤ºè°ƒç”¨å®Œæˆ")
            except Exception as show_error:
                logger.warning(f"å›¾è¡¨æ˜¾ç¤ºæ—¶å‡ºç°è­¦å‘Š: {str(show_error)}")
            
            logger.info("å›¾è¡¨ç»˜åˆ¶æµç¨‹å®Œæˆ")
            return plot_path
            
        except Exception as e:
            logger.error(f"ç»˜åˆ¶å›¾è¡¨å¤±è´¥: {str(e)}")
            logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", exc_info=True)
            return None
    
    def save_prediction_results(self, pred_df, symbol, metadata=None, is_future_forecast=False):
        """
        ä¿å­˜é¢„æµ‹ç»“æœ
        
        Args:
            pred_df (pd.DataFrame): é¢„æµ‹ç»“æœ
            symbol (str): è‚¡ç¥¨ä»£ç 
            metadata (dict): å…ƒæ•°æ®
            
        Returns:
            str: ç»“æœæ–‡ä»¶è·¯å¾„
        """
        try:
            # --- åˆ›å»ºæ¨¡å¼åŒºåˆ†çš„ç»“æœå­æ–‡ä»¶å¤¹ ---
            mode_dir = 'future_forecast' if is_future_forecast else 'backtest'
            symbol_results_dir = os.path.join(self.results_dir, symbol, mode_dir)
            os.makedirs(symbol_results_dir, exist_ok=True)

            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            mode_prefix = 'forecast' if is_future_forecast else 'backtest'

            # ä¿å­˜é¢„æµ‹æ•°æ®
            csv_filename = f"{symbol}_{mode_prefix}_data_{timestamp}.csv"
            csv_path = os.path.join(symbol_results_dir, csv_filename)
            
            # å°†ç´¢å¼•é‡ç½®ä¸ºåˆ—ï¼Œå¹¶ç¡®ä¿åˆ—åä¸º'timestamps'
            save_df = pred_df.reset_index()
            if 'index' in save_df.columns:
                save_df = save_df.rename(columns={'index': 'timestamps'})
            
            save_df.to_csv(csv_path, index=False)
            
            # ä¿å­˜å…ƒæ•°æ®
            if metadata is None:
                metadata = {}
            
            metadata.update({
                'symbol': symbol,
                'prediction_time': timestamp,
                'data_points': len(pred_df),
                'columns': list(pred_df.columns)
            })
            
            json_filename = f"{symbol}_{mode_prefix}_metadata_{timestamp}.json"
            json_path = os.path.join(symbol_results_dir, json_filename)
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2, default=str)
            
            mode_desc = "æœªæ¥é¢„æµ‹" if is_future_forecast else "å†å²å›æµ‹"
            print(f"âœ… {mode_desc}ç»“æœå·²ä¿å­˜è‡³ä¸“é—¨æ–‡ä»¶å¤¹:")
            print(f"   ğŸ“‚ ç›®å½•: {symbol_results_dir}")
            print(f"   ğŸ“„ æ•°æ®æ–‡ä»¶: {os.path.basename(csv_path)}")
            print(f"   ğŸ“‹ å…ƒæ•°æ®æ–‡ä»¶: {os.path.basename(json_path)}")
            print(f"   ğŸ¯ æ¨¡å¼: {mode_desc}")
            
            return csv_path
            
        except Exception as e:
            print(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return None
    
    def analyze_prediction(self, historical_df, pred_df, symbol, is_future_forecast):
        """
        å…¨é¢åˆ†æé¢„æµ‹ç»“æœ
        """
        try:
            logger.info("å¼€å§‹è¯¦ç»†åˆ†æé¢„æµ‹ç»“æœ...")

            # è·å–å†å²æ•°æ®çš„æœ€åä¸€ä¸ªç‚¹ç”¨äºæ¯”è¾ƒ
            last_historical_point = historical_df.iloc[-1]
            last_close = last_historical_point['close']
            
            # === 1. åŸºç¡€ç»Ÿè®¡åˆ†æ ===
            pred_close_stats = self._calculate_price_statistics(pred_df)
            pred_volume_stats = self._calculate_volume_statistics(pred_df)

            # === 2. è¶‹åŠ¿å’Œå˜åŒ–åˆ†æ ===
            trend_analysis = self._analyze_trend_and_changes(historical_df, pred_df, is_future_forecast)

            # === 3. é£é™©å’Œæ³¢åŠ¨æ€§åˆ†æ ===
            risk_analysis = self._analyze_risk_and_volatility(historical_df, pred_df)

            # === 4. é¢„æµ‹è´¨é‡è¯„ä¼° ===
            quality_metrics = self._evaluate_prediction_quality(historical_df, pred_df, is_future_forecast)

            # === 5. æŠ€æœ¯æŒ‡æ ‡åˆ†æ ===
            technical_analysis = self._calculate_technical_indicators(pred_df, historical_df)

            # æ•´åˆæ‰€æœ‰åˆ†æç»“æœ
            analysis = {
                'symbol': symbol,
                'historical_last_close': last_close,
                'prediction_period_days': len(pred_df),
                'is_future_forecast': is_future_forecast,
                'price_analysis': {
                    **pred_close_stats,
                    **trend_analysis['price']
                },
                'volume_analysis': pred_volume_stats,
                'risk_analysis': risk_analysis,
                'quality_metrics': quality_metrics,
                'technical_analysis': technical_analysis,
                'timestamp': datetime.now().isoformat()
            }

            logger.info(f"é¢„æµ‹åˆ†æå®Œæˆ - è¶‹åŠ¿: {analysis['price_analysis']['trend']}, "
                       f"é¢„æœŸå˜åŒ–: {analysis['price_analysis']['price_change_percentage']:.2f}%")
            return analysis

        except Exception as e:
            logger.error(f"é¢„æµ‹ç»“æœåˆ†æå¤±è´¥: {str(e)}")
            return None

    def _calculate_price_statistics(self, pred_df):
        """è®¡ç®—ä»·æ ¼åŸºç¡€ç»Ÿè®¡"""
        close_prices = pred_df['close']

        return {
            'mean': close_prices.mean(),
            'std': close_prices.std(),
            'min': close_prices.min(),
            'max': close_prices.max(),
            'median': close_prices.median(),
            'q25': close_prices.quantile(0.25),
            'q75': close_prices.quantile(0.75),
            'range': close_prices.max() - close_prices.min(),
            'cv': close_prices.std() / close_prices.mean() if close_prices.mean() != 0 else 0
        }

    def _calculate_volume_statistics(self, pred_df):
        """è®¡ç®—æˆäº¤é‡ç»Ÿè®¡"""
        if 'volume' not in pred_df.columns:
            return {'trend': 'æ— æ•°æ®', 'avg_volume': 0}

        volumes = pred_df['volume']

        return {
            'mean': volumes.mean(),
            'std': volumes.std(),
            'min': volumes.min(),
            'max': volumes.max(),
            'trend': 'å¢åŠ ' if volumes.iloc[-1] > volumes.iloc[0] else 'å‡å°‘',
            'volatility': volumes.std() / volumes.mean() if volumes.mean() != 0 else 0
        }

    def _analyze_trend_and_changes(self, historical_df, pred_df, is_future_forecast):
        """åˆ†æè¶‹åŠ¿å’Œå˜åŒ–"""
        # ç¡®å®šæ¯”è¾ƒåŸºå‡†ç‚¹
        if is_future_forecast:
            baseline_close = historical_df.iloc[-1]['close']
        else:
            comparison_point = historical_df[historical_df['timestamps'] < pred_df.index.min()]
            baseline_close = comparison_point.iloc[-1]['close'] if not comparison_point.empty else historical_df.iloc[-1]['close']

        # è®¡ç®—é¢„æµ‹çš„å˜åŒ–
        pred_start = pred_df['close'].iloc[0]
        pred_end = pred_df['close'].iloc[-1]

        price_change = pred_end - baseline_close
        price_change_pct = (price_change / baseline_close) * 100 if baseline_close != 0 else 0

        # è¶‹åŠ¿å¼ºåº¦åˆ†æ
        pred_trend_strength = abs(pred_end - pred_start) / pred_start if pred_start != 0 else 0

        return {
            'price': {
                'baseline_close': baseline_close,
                'pred_start': pred_start,
                'pred_end': pred_end,
                    'price_change': price_change,
                    'price_change_percentage': price_change_pct,
                'trend': 'ä¸Šæ¶¨' if pred_end > pred_start else 'ä¸‹è·Œ',
                'trend_strength': pred_trend_strength,
                'direction_consistency': 'ä¸€è‡´' if (pred_end > pred_start) == (price_change > 0) else 'ä¸ä¸€è‡´'
            }
        }

    def _analyze_risk_and_volatility(self, historical_df, pred_df):
        """åˆ†æé£é™©å’Œæ³¢åŠ¨æ€§"""
        # è®¡ç®—é¢„æµ‹æ•°æ®çš„æ³¢åŠ¨æ€§
        pred_returns = pred_df['close'].pct_change().dropna()
        pred_volatility = pred_returns.std() * (252 ** 0.5)  # å¹´åŒ–æ³¢åŠ¨ç‡

        # è®¡ç®—å†å²æ•°æ®çš„æ³¢åŠ¨æ€§ä½œä¸ºå¯¹æ¯”
        hist_returns = historical_df['close'].pct_change().dropna().tail(100)  # æœ€è¿‘100ä¸ªç‚¹
        hist_volatility = hist_returns.std() * (252 ** 0.5) if len(hist_returns) > 0 else 0

        # é£é™©æŒ‡æ ‡
        var_95 = np.percentile(pred_returns, 5)  # 95% VaR
        max_drawdown = self._calculate_max_drawdown(pred_df['close'])

        return {
            'pred_volatility': pred_volatility,
            'hist_volatility': hist_volatility,
            'volatility_ratio': pred_volatility / hist_volatility if hist_volatility != 0 else float('inf'),
            'var_95': var_95,
            'max_drawdown': max_drawdown,
            'sharpe_ratio': pred_returns.mean() / pred_returns.std() if pred_returns.std() != 0 else 0
        }

    def _evaluate_prediction_quality(self, historical_df, pred_df, is_future_forecast):
        """è¯„ä¼°é¢„æµ‹è´¨é‡"""
        quality_scores = {}

        # 1. å¹³æ»‘æ€§è¯„åˆ† (0-1, 1è¡¨ç¤ºéå¸¸å¹³æ»‘)
        returns = pred_df['close'].pct_change().dropna()
        smoothness = 1 / (1 + returns.std())  # æ³¢åŠ¨ç‡è¶Šä½ï¼Œå¹³æ»‘æ€§è¶Šé«˜
        quality_scores['smoothness'] = smoothness

        # 2. åˆç†æ€§è¯„åˆ† (åŸºäºå†å²æ³¢åŠ¨æ€§)
        hist_volatility = historical_df['close'].pct_change().dropna().tail(50).std()
        pred_volatility = returns.std()

        if hist_volatility > 0:
            volatility_ratio = pred_volatility / hist_volatility
            # ç†æƒ³çš„æ³¢åŠ¨ç‡æ¯”åº”è¯¥åœ¨0.5-2.0ä¹‹é—´
            if 0.5 <= volatility_ratio <= 2.0:
                reasonableness = 1.0
            else:
                reasonableness = max(0, 1 - abs(np.log(volatility_ratio)))
        else:
            reasonableness = 0.5

        quality_scores['reasonableness'] = reasonableness

        # 3. è¿ç»­æ€§è¯„åˆ† (æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸è·³è·ƒ)
        jumps = (returns.abs() > 0.1).sum()  # å•æ­¥å˜åŒ–è¶…è¿‡10%çš„æ¬¡æ•°
        continuity = max(0, 1 - jumps / len(returns))
        quality_scores['continuity'] = continuity

        # 4. æ•´ä½“è´¨é‡è¯„åˆ†
        quality_scores['overall_score'] = np.mean([smoothness, reasonableness, continuity])

        return quality_scores

    def _calculate_technical_indicators(self, pred_df, historical_df=None):
        """
        è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        å¦‚æœæä¾›äº†historical_dfï¼Œä¼šå°†å…¶ä¸pred_dfåˆå¹¶ä»¥è®¡ç®—æ›´å‡†ç¡®çš„æŒ‡æ ‡ï¼ˆå¦‚MA20éœ€è¦è‡³å°‘20å¤©æ•°æ®ï¼‰
        """
        indicators = {}
        
        # å‡†å¤‡è®¡ç®—ç”¨çš„æ•°æ®
        if historical_df is not None and not historical_df.empty:
            # å–å†å²æ•°æ®çš„æœ€åä¸€éƒ¨åˆ†ï¼Œç¡®ä¿è¶³å¤Ÿè®¡ç®—é•¿å‘¨æœŸæŒ‡æ ‡ (å¦‚MACDéœ€è¦26+9=35å¤©ï¼ŒMA60éœ€è¦60å¤©)
            # å–100å¤©åº”è¯¥è¶³å¤Ÿ
            hist_subset = historical_df.tail(100)[['close']].copy()
            pred_subset = pred_df[['close']].copy()
            combined_df = pd.concat([hist_subset, pred_subset])
        else:
            combined_df = pred_df[['close']].copy()

        if len(combined_df) < 5:  # æ•°æ®ç‚¹å¤ªå°‘
            return indicators

        try:
            # ä½¿ç”¨TechnicalAnalyzerè®¡ç®—
            # MA
            ma5 = TechnicalAnalyzer.calculate_ma(combined_df['close'], 5)
            ma10 = TechnicalAnalyzer.calculate_ma(combined_df['close'], 10)
            ma20 = TechnicalAnalyzer.calculate_ma(combined_df['close'], 20)
            
            # MACD
            macd, signal, hist = TechnicalAnalyzer.calculate_macd(combined_df['close'])
            
            # RSI
            rsi = TechnicalAnalyzer.calculate_rsi(combined_df['close'])
            
            # è·å–é¢„æµ‹éƒ¨åˆ†çš„æœ€åä¸€ä¸ªå€¼
            indicators['ma5'] = ma5.iloc[-1]
            indicators['ma10'] = ma10.iloc[-1]
            indicators['ma20'] = ma20.iloc[-1]
            
            indicators['macd'] = macd.iloc[-1]
            indicators['macd_signal'] = signal.iloc[-1]
            indicators['macd_hist'] = hist.iloc[-1]
            
            indicators['rsi'] = rsi.iloc[-1]
            
            # æ·»åŠ è¶‹åŠ¿åˆ¤æ–­
            indicators['trend_ma'] = 'bullish' if ma5.iloc[-1] > ma10.iloc[-1] > ma20.iloc[-1] else 'bearish'
            indicators['trend_macd'] = 'bullish' if macd.iloc[-1] > signal.iloc[-1] else 'bearish'
            indicators['trend_rsi'] = 'overbought' if rsi.iloc[-1] > 70 else ('oversold' if rsi.iloc[-1] < 30 else 'neutral')
            
        except Exception as e:
            self.logger.warning(f"è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
            
        return indicators

    def _calculate_max_drawdown(self, prices):
        """è®¡ç®—æœ€å¤§å›æ’¤"""
        peak = prices.expanding().max()
        drawdown = (prices - peak) / peak
        return drawdown.min()
    
    def run_prediction_pipeline(self, historical_df, x_df, x_timestamp, y_timestamp,
                               is_future_forecast, symbol, pred_len,
                               T=1.0, top_p=0.9, sample_count=1, plot_lookback=1500,
                               enable_advanced_preprocessing=False, price_normalization="none",
                               trend_adjustment=False, volatility_filter=False, config=None):
        """
        è¿è¡Œå®Œæ•´çš„é¢„æµ‹æµç¨‹

        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«å›¾è¡¨æ˜¾ç¤ºç­‰è®¾ç½®
        """
        mode_name = "æœªæ¥é¢„æµ‹" if is_future_forecast else "å†å²å›æµ‹"
        logger.info(f"ğŸš€ å¼€å§‹ {symbol} çš„{mode_name}æµç¨‹...")

        try:
            # === æ•°æ®éªŒè¯å’Œé¢„å¤„ç† ===
            logger.info("ğŸ“Š æ•°æ®éªŒè¯å’Œé¢„å¤„ç†...")

            # éªŒè¯å†å²æ•°æ®
            is_valid, error_msg = self.validate_data(historical_df, "historical_data")
            if not is_valid:
                logger.error(f"å†å²æ•°æ®éªŒè¯å¤±è´¥: {error_msg}")
                return None

            # éªŒè¯è¾“å…¥æ•°æ®
            input_df = x_df.copy()
            input_df[self.data_config['timestamp_column']] = x_timestamp
            is_valid, error_msg = self.validate_data(input_df, "input_data")
            if not is_valid:
                logger.error(f"è¾“å…¥æ•°æ®éªŒè¯å¤±è´¥: {error_msg}")
                return None

            # é¢„å¤„ç†å†å²æ•°æ®
            historical_df = self.preprocess_data(
                historical_df,
                enable_advanced=enable_advanced_preprocessing,
                normalization=price_normalization,
                trend_adjustment=trend_adjustment,
                volatility_filter=volatility_filter
            )
            x_df = self.preprocess_data(
                input_df,
                enable_advanced=enable_advanced_preprocessing,
                normalization=price_normalization,
                trend_adjustment=trend_adjustment,
                volatility_filter=volatility_filter
            )[REQUIRED_COLUMNS]

            # ç¡®ä¿ y_timestamp æ˜¯æ­£ç¡®çš„ç±»å‹
            y_timestamp_series = pd.Series(pd.to_datetime(y_timestamp))

            # === 1. è¿›è¡Œé¢„æµ‹ ===
            logger.info("ğŸ”® å¼€å§‹æ¨¡å‹æ¨ç†...")
            pred_df = self.predict(x_df, x_timestamp, y_timestamp_series, pred_len, T, top_p, sample_count)
            if pred_df is None:
                logger.error("é¢„æµ‹å¤±è´¥ï¼Œè¿”å›None")
                return None

            # === 2. ç¡®ä¿ pred_df çš„ç´¢å¼•æ˜¯ DatetimeIndex ===
            pred_df.index = pd.to_datetime(pred_df.index)

            # === 3. åˆ†æé¢„æµ‹ç»“æœ ===
            logger.info("ğŸ“ˆ åˆ†æé¢„æµ‹ç»“æœ...")
            analysis = self.analyze_prediction(historical_df, pred_df, symbol, is_future_forecast)
            if analysis is None:
                logger.error("é¢„æµ‹ç»“æœåˆ†æå¤±è´¥")
                return None

            # === 4. ç»˜åˆ¶å›¾è¡¨ ===
            logger.info("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
            plot_path = self.plot_prediction(
                historical_df, pred_df, symbol, is_future_forecast,
                plot_lookback=plot_lookback,
                enable_focus_mode=config.get('enable_focus_mode', False) if config else False,
                plot_lookback_days=config.get('plot_lookback_days') if config else None,
                prediction_highlight=config.get('prediction_highlight', True) if config else True,
                raw_historical_df=historical_df  # ä¼ å…¥åŸå§‹å†å²æ•°æ®ç”¨äºæ­£ç¡®æ˜¾ç¤ºä»·æ ¼å°ºåº¦
            )
            if plot_path is None:
                logger.error("âŒ å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼Œplot_pathä¸ºNone")
            else:
                logger.info(f"âœ… å›¾è¡¨ç”ŸæˆæˆåŠŸ: {plot_path}")

            # === 5. ä¿å­˜ç»“æœ ===
            logger.info("ğŸ’¾ ä¿å­˜é¢„æµ‹ç»“æœ...")
            metadata = {
                'analysis': analysis,
                'plot_path': plot_path,
                'parameters': {
                    'pred_len': pred_len,
                    'T': T,
                    'top_p': top_p,
                    'sample_count': sample_count,
                    'is_future_forecast': is_future_forecast
                },
                'data_quality': {
                    'input_points': len(x_df),
                    'prediction_points': len(pred_df),
                    'processing_timestamp': datetime.now().isoformat()
                }
            }

            csv_path = self.save_prediction_results(pred_df, symbol, metadata, is_future_forecast)

            # === 6. è¿”å›å®Œæ•´ç»“æœ ===
            results = {
                'symbol': symbol,
                'prediction': pred_df,
                'analysis': analysis,
                'files': {
                    'csv_path': csv_path,
                    'plot_path': plot_path
                },
                'metadata': metadata
            }

            logger.info(f"ğŸ‰ {mode_name}æµç¨‹å…¨éƒ¨å®Œæˆï¼")
            logger.info(f"ğŸ“ˆ {mode_name}å›¾è¡¨å·²ä¿å­˜è‡³: {plot_path}")
            logger.info(f"ğŸ“„ {mode_name}æ•°æ®å·²ä¿å­˜è‡³: {csv_path}")

            return results

        except Exception as e:
            logger.error(f"é¢„æµ‹æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}")
            logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:", exc_info=True)
            return None

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="Kronos è‚¡ç¥¨é¢„æµ‹å™¨ - ç‹¬ç«‹è¿è¡Œç‰ˆæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python stock_predictor.py                                    # ä½¿ç”¨é»˜è®¤è®¾ç½®è¿è¡Œç¤ºä¾‹
  python stock_predictor.py --device cpu                      # ä½¿ç”¨ CPU è¿è¡Œ
  python stock_predictor.py --device cuda:0                   # ä½¿ç”¨ GPU è¿è¡Œ
  python stock_predictor.py --data-path /path/to/data.csv --symbol 000001  # ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®

å‚æ•°è¯´æ˜:
  device: è®¡ç®—è®¾å¤‡é€‰æ‹©
    - auto: è‡ªåŠ¨æ£€æµ‹ (é»˜è®¤ï¼Œæ¨è)
    - cpu: ä½¿ç”¨ CPU
    - cuda:0: ä½¿ç”¨ç¬¬ä¸€ä¸ª CUDA GPU
    - mps: ä½¿ç”¨ Apple Silicon GPU

  data-path: è‡ªå®šä¹‰æ•°æ®æ–‡ä»¶è·¯å¾„ (å¯é€‰)
    å¦‚æœä¸æŒ‡å®šï¼Œå°†ä½¿ç”¨é¡¹ç›®ä¸­çš„ç¤ºä¾‹æ•°æ®

  symbol: è‚¡ç¥¨ä»£ç  (å¯é€‰)
    ä¸ data-path é…åˆä½¿ç”¨ï¼Œé»˜è®¤ '600977'
        """
    )

    parser.add_argument(
        "--device", "-d",
        default="auto",
        choices=["auto", "cpu", "cuda", "cuda:0", "mps"],
        help="è®¡ç®—è®¾å¤‡ (é»˜è®¤: auto)"
    )

    parser.add_argument(
        "--data-path",
        help="è‡ªå®šä¹‰æ•°æ®æ–‡ä»¶è·¯å¾„ (CSVæ ¼å¼ï¼ŒåŒ…å«OHLCVæ•°æ®)"
    )

    parser.add_argument(
        "--symbol", "-s",
        default="600977",
        help="è‚¡ç¥¨ä»£ç  (é»˜è®¤: 600977)"
    )

    parser.add_argument(
        "--lookback", "-l",
        type=int,
        default=1500,
        help="å†å²æ•°æ®ç‚¹æ•°é‡ (é»˜è®¤: 1500)"
    )

    parser.add_argument(
        "--pred-len", "-p",
        type=int,
        default=96,
        help="é¢„æµ‹æ•°æ®ç‚¹æ•°é‡ (é»˜è®¤: 96)"
    )

    parser.add_argument(
        "--future-forecast",
        action="store_true",
        help="æœªæ¥é¢„æµ‹æ¨¡å¼ (é»˜è®¤: Falseï¼Œå›æµ‹æ¨¡å¼)"
    )

    return parser.parse_args()

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    print("="*60)
    print("ğŸ¯ Kronos è‚¡ç¥¨é¢„æµ‹å™¨ - ç‹¬ç«‹è¿è¡Œç‰ˆæœ¬")
    print("="*60)
    print(f"ğŸ“‹ ä½¿ç”¨å‚æ•°: device={args.device}, symbol={args.symbol}")

    # 1. åˆ›å»ºé¢„æµ‹å™¨
    try:
        print(f"\nğŸš€ æ­£åœ¨åˆå§‹åŒ–é¢„æµ‹å™¨ (device: {args.device})...")
        predictor = StockPredictor(device=args.device)
        print("âœ… é¢„æµ‹å™¨åˆå§‹åŒ–æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ é¢„æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        print("\nğŸ”§ å¯èƒ½çš„åŸå› :")
        print("  1. ç¼ºå°‘ä¾èµ–åŒ…ï¼Œè¯·è¿è¡Œ: pip install -r requirements.txt")
        print("  2. Kronos æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print(f"  3. è®¾å¤‡ '{args.device}' ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨ 'cpu'")
        print("\nğŸ’¡ å»ºè®®:")
        print("  python stock_predictor.py --device cpu")
        return

    # 2. ç¡®å®šæ•°æ®æ–‡ä»¶è·¯å¾„
    if args.data_path:
        data_path = args.data_path
        symbol = args.symbol
        print(f"\nğŸ“‚ ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®æ–‡ä»¶: {data_path}")
        print(f"ğŸ“ˆ è‚¡ç¥¨ä»£ç : {symbol}")
    else:
        # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
        data_path = os.path.join("examples", "data", "XSHG_5min_600977.csv")
        symbol = "600977"
        print(f"\nğŸ“‚ ä½¿ç”¨ç¤ºä¾‹æ•°æ®æ–‡ä»¶: {data_path}")
        print(f"ğŸ“ˆ è‚¡ç¥¨ä»£ç : {symbol}")

    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        print("\nğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
        if args.data_path:
            print("  1. æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
            print("  2. ç¡®ä¿æ–‡ä»¶åŒ…å«å¿…è¦çš„åˆ—: timestamps, open, high, low, close, volume, amount")
        else:
            print("  1. è¿è¡Œæ•°æ®è·å–è„šæœ¬è·å–è‚¡ç¥¨æ•°æ®:")
            print("     python my_stock_predictor/run_my_prediction.py")
        return

    # 3. åŠ è½½æ•°æ®
    print(f"\nğŸ“– æ­£åœ¨åŠ è½½æ•°æ®...")
    df = predictor.load_data(data_path)
    if df is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return

    # 4. å‡†å¤‡é¢„æµ‹æ•°æ®
    lookback = args.lookback
    pred_len = args.pred_len
    is_future_forecast = args.future_forecast

    print(f"\nâš™ï¸ é¢„æµ‹å‚æ•°:")
    print(f"   - å†å²æ•°æ®ç‚¹: {lookback}")
    print(f"   - é¢„æµ‹é•¿åº¦: {pred_len}")
    print(f"   - é¢„æµ‹æ¨¡å¼: {'æœªæ¥é¢„æµ‹' if is_future_forecast else 'å†å²å›æµ‹'}")

    # æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿ
    if len(df) < lookback + pred_len:
        print(f"âš ï¸ è­¦å‘Š: æ•°æ®ç‚¹ä¸è¶³ (éœ€è¦ {lookback + pred_len}, å®é™… {len(df)})")
        # è‡ªåŠ¨è°ƒæ•´å‚æ•°
        available_points = len(df)
        lookback = min(lookback, available_points // 2)
        pred_len = min(pred_len, available_points - lookback)
        print(f"ğŸ”§ è‡ªåŠ¨è°ƒæ•´å‚æ•°: lookback={lookback}, pred_len={pred_len}")

    x_df, x_timestamp, y_timestamp = predictor.prepare_prediction_data(df, lookback, pred_len)

    # 5. è¿è¡Œé¢„æµ‹æµç¨‹
    print("\nğŸ”® å¼€å§‹é¢„æµ‹æµç¨‹...")
    start_time = datetime.now()

    results = predictor.run_prediction_pipeline(
        historical_df=df,
        x_df=x_df,
        x_timestamp=x_timestamp,
        y_timestamp=y_timestamp,
        is_future_forecast=is_future_forecast,
        symbol=symbol,
        pred_len=pred_len,
        T=1.0,
        top_p=0.9,
        sample_count=1,
        plot_lookback=lookback
    )

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    if results:
        print(f"\nğŸ‰ é¢„æµ‹å®Œæˆï¼ç”¨æ—¶ {duration:.1f} ç§’")
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ° prediction_results/{symbol}/ ç›®å½•")
        print("   - æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨å’Œæ•°æ®æ–‡ä»¶")
        # æ‰“å°ç»“æœæ¦‚è§ˆ
        analysis = results.get('analysis', {})
        if analysis:
            price_change_pct = analysis.get('price_analysis', {}).get('price_change_percentage', 0)
            trend = analysis.get('price_analysis', {}).get('trend', 'æœªçŸ¥')
            print("\nğŸ“Š é¢„æµ‹æ¦‚è§ˆ:")
            print(f"   - ä»·æ ¼å˜åŒ–: {price_change_pct:.2f}%")
            print(f"   - è¶‹åŠ¿: {trend}")
    else:
        print(f"\nâŒ é¢„æµ‹æµç¨‹å¤±è´¥ï¼Œç”¨æ—¶ {duration:.1f} ç§’")

if __name__ == "__main__":
    main()
